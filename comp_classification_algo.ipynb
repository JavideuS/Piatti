{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcuNuqTXYZDI",
        "outputId": "56a2e7c2-e2e6-432c-a6f0-e046710f79c8"
      },
      "outputs": [],
      "source": [
        "# Assigment 1: Comparative Study of Classification Algorithms.\n",
        "print(\"Hola mundo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQd5APwtaJgd",
        "outputId": "11adf735-feb3-4499-b33c-f5ba25e55d73"
      },
      "outputs": [],
      "source": [
        "print(\"Mundo dice ciao\")\n",
        "print(\"Ciao dice mundo\")\n",
        "print(\"Mundo dice que ciao lava piati\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL71bsABbaqg",
        "outputId": "dbe0a991-96eb-45fd-9776-a3f51d27a853"
      },
      "outputs": [],
      "source": [
        "print(\"Mundo gioca calcio\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E61_93x60v2",
        "outputId": "2b972ac9-e26f-4585-f991-1538b798f08f"
      },
      "outputs": [],
      "source": [
        "print(\"Giusseppe se puso la peluca\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDE8NrPXKdZ2",
        "outputId": "7a767fe5-7c62-460c-db86-93e7da7c95f0"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Dataset directory\n",
        "dataset_dir = \"/content/data/raw\"\n",
        "\n",
        "# Download dataset\n",
        "path = kagglehub.dataset_download(\"jiayuanchengala/aid-scene-classification-datasets\")\n",
        "src_dir = os.path.join(path, \"AID\")\n",
        "\n",
        "# Move only if not already moved\n",
        "if not os.path.exists(dataset_dir):\n",
        "    print(\"Copying dataset to /content/data/raw...\")\n",
        "    # Note that if I move it then kagglehub wont re-download so we can=t keep consistency\n",
        "    shutil.copytree(src_dir, dataset_dir)\n",
        "else:\n",
        "    print(\"Dataset already exists at /content/data/raw\")\n",
        "\n",
        "print(\"✅ Dataset ready at:\", dataset_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOrWQiOhTJMZ"
      },
      "source": [
        "**Manual Data Separation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hueuKpDe6uw9",
        "outputId": "2e6a1e7e-3ee0-42cc-f2c4-fd0b542aaf57"
      },
      "outputs": [],
      "source": [
        "image_paths =  []\n",
        "labels = []\n",
        "source_dir = 'data/raw'\n",
        "categories = [d for d in os.listdir(source_dir)\n",
        "              if os.path.isdir(os.path.join(source_dir, d))]\n",
        "for category in categories:\n",
        "    category_path = os.path.join(source_dir, category)\n",
        "    files = [os.path.join(category_path, f) for f in os.listdir(category_path)\n",
        "             if os.path.isfile(os.path.join(category_path, f))]\n",
        "    image_paths.extend(files)\n",
        "    labels.extend([category] * len(files))\n",
        "\n",
        "print(f\"Found {len(image_paths)} images in {len(categories)} categories\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCZYPpJOU1kV",
        "outputId": "a618bebc-3738-47a6-cc44-e1ec70ed9a0a"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Source directory containing all the category folders\n",
        "source_dir = 'data/raw'\n",
        "\n",
        "# Create train and test directories\n",
        "train_dir = 'data/train'\n",
        "test_dir = 'data/test'\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "for category in categories:\n",
        "    category_path = os.path.join(source_dir, category)\n",
        "\n",
        "    if not os.path.exists(category_path):\n",
        "        print(f\"Warning: {category} folder not found, skipping...\")\n",
        "        continue\n",
        "\n",
        "    # Get all files in the category folder\n",
        "    files = [f for f in os.listdir(category_path)\n",
        "             if os.path.isfile(os.path.join(category_path, f))]\n",
        "\n",
        "    if len(files) == 0:\n",
        "        print(f\"Warning: {category} folder is empty, skipping...\")\n",
        "        continue\n",
        "\n",
        "    # Shuffle files randomly\n",
        "    random.shuffle(files)\n",
        "\n",
        "    # Calculate split point (90% for train)\n",
        "    split_idx = int(len(files) * 0.9)\n",
        "    train_files = files[:split_idx]\n",
        "    test_files = files[split_idx:]\n",
        "\n",
        "    # Create category subfolders in train and test\n",
        "    train_category_dir = os.path.join(train_dir, category)\n",
        "    test_category_dir = os.path.join(test_dir, category)\n",
        "    os.makedirs(train_category_dir, exist_ok=True)\n",
        "    os.makedirs(test_category_dir, exist_ok=True)\n",
        "\n",
        "    # Move files to train\n",
        "    for file in train_files:\n",
        "        src = os.path.join(category_path, file)\n",
        "        dst = os.path.join(train_category_dir, file)\n",
        "        shutil.copy2(src, dst)\n",
        "\n",
        "    # Move files to test\n",
        "    for file in test_files:\n",
        "        src = os.path.join(category_path, file)\n",
        "        dst = os.path.join(test_category_dir, file)\n",
        "        shutil.copy2(src, dst)\n",
        "\n",
        "    print(f\"{category}: {len(train_files)} files to train, {len(test_files)} files to test\")\n",
        "\n",
        "print(\"\\nSplit complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GCdL4xacn_y",
        "outputId": "c7a6eb9c-f47d-481a-f0d9-3bedc6078a50"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# This is for cross-validation (Pytorch and Tensorflow need manual separation)\n",
        "# To scikit-learning you onl need to pass the training data and it does the division\n",
        "\n",
        "# Configuration\n",
        "k_folds = 5\n",
        "output_base = 'kfolds'\n",
        "os.makedirs(output_base, exist_ok=True)\n",
        "\n",
        "print(f\"Creating {k_folds}-fold cross-validation split...\")\n",
        "\n",
        "# Collect all files by category\n",
        "category_files = {}\n",
        "for category in categories:\n",
        "    category_path = os.path.join(source_dir, category)\n",
        "    files = [f for f in os.listdir(category_path)\n",
        "             if os.path.isfile(os.path.join(category_path, f))]\n",
        "    random.shuffle(files)  # Shuffle within each category\n",
        "    category_files[category] = files\n",
        "\n",
        "# Create fold assignments for each category\n",
        "fold_assignments = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "for category, files in category_files.items():\n",
        "    n_files = len(files)\n",
        "    fold_size = n_files // k_folds\n",
        "\n",
        "    # Assign files to folds ensuring balanced distribution\n",
        "    for fold_idx in range(k_folds):\n",
        "        start_idx = fold_idx * fold_size\n",
        "        # Last fold gets any remaining files\n",
        "        end_idx = start_idx + fold_size if fold_idx < k_folds - 1 else n_files\n",
        "        fold_assignments[fold_idx][category] = files[start_idx:end_idx]\n",
        "\n",
        "# Create fold directories with class subdirectories only\n",
        "for fold_idx in range(k_folds):\n",
        "    fold_name = f'fold_{fold_idx + 1}'\n",
        "    fold_dir = os.path.join(output_base, fold_name)\n",
        "\n",
        "    # Create class subdirectories in each fold\n",
        "    for category in categories:\n",
        "        category_fold_dir = os.path.join(fold_dir, category)\n",
        "        os.makedirs(category_fold_dir, exist_ok=True)\n",
        "\n",
        "        # Copy the files assigned to this fold for this category\n",
        "        files_for_this_fold = fold_assignments[fold_idx][category]\n",
        "\n",
        "        for file in files_for_this_fold:\n",
        "            src = os.path.join(source_dir, category, file)\n",
        "            dst = os.path.join(category_fold_dir, file)\n",
        "            shutil.copy2(src, dst)\n",
        "\n",
        "    print(f\"\\n{fold_name}:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    total_files = 0\n",
        "    for category in categories:\n",
        "        files_in_fold = fold_assignments[fold_idx][category]\n",
        "        total_files += len(files_in_fold)\n",
        "        print(f\"  {category}: {len(files_in_fold)} files\")\n",
        "\n",
        "    print(f\"  Total in fold: {total_files} files\")\n",
        "\n",
        "print(f\"\\n\\nK-fold split complete!\")\n",
        "print(f\"Output directory: {output_base}/\")\n",
        "print(f\"Structure: fold_1/, fold_2/, ..., fold_{k_folds}/\")\n",
        "print(f\"Each fold contains: {', '.join(categories)} subdirectories with their respective files\")\n",
        "\n",
        "print(f\"\\n\\nK-fold split complete!\")\n",
        "print(f\"Output directory: {output_base}/\")\n",
        "print(f\"Structure: fold_1/, fold_2/, ..., fold_{k_folds}/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvDO7tPCTiTM"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEk296AuPYxq"
      },
      "outputs": [],
      "source": [
        "# Since scikit-learning doesn't have features extraction for images\n",
        "# We have to use also openCV to create our dictionary of features\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(image_paths, labels, test_size=0.2, random_state=0)\n",
        "\n",
        "# We get all descriptors for training set before kfold, does not count as data leakage\n",
        "# It just avoids calculating them k times\n",
        "orb = cv.ORB_create() # Fast but poor performance\n",
        "# sift = cv.SIFT_create(nfeatures=2000) #13gb is not enough for sift(40+ min)\n",
        "sift = cv.SIFT_create()\n",
        "# surf = cv.xfeatures2d.SURF_create() Non-free\n",
        "akaze = cv.AKAZE_create(descriptor_type=cv.AKAZE_DESCRIPTOR_MLDB,\n",
        "                         descriptor_size=0,   # 486 bits\n",
        "                         threshold=0.001) #Slightly faster same results\n",
        "fast = cv.FastFeatureDetector_create()\n",
        "\n",
        "#Sift with 500 features performs a faster than akaze and just has 2% less accuracy (10 min training and faster kmeans)\n",
        "# 2000 features takes 8gb rams and takes around 35-40min, but can't do kmeans)\n",
        "# 1500 features takes 6.7 and around 35 min, but still isnt enough for kmeans\n",
        "# 1000 features takes 5gb ram and around 31 min, but still doesnt work with kmeans\n",
        "# Kmeans consumes around 6gb ram, so any detector method that keeps more than 6gb too will probably crash\n",
        "\n",
        "# Note that this also affect the kmeans\n",
        "# Seems like more computing time on getting labels is also translated to longer kmean\n",
        "\n",
        "image_descriptors = []\n",
        "pca = PCA(n_components=64) # Reduce descriptor dimension by half from 128 to 64\n",
        "output_dir = \"descriptors\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for idx, x in enumerate(X_train):\n",
        "  img = cv.imread(x, cv.IMREAD_GRAYSCALE)\n",
        "  keypoints = sift.detect(img, None)\n",
        "  keypoints, descriptors = sift.compute(img, keypoints)\n",
        "  # Reducing precision from float32 to float16\n",
        "  # if descriptors is not None:\n",
        "    # descriptors = descriptors.astype('float16')\n",
        "    # descriptors = pca.fit_transform(descriptors)\n",
        "  np.save(f\"{output_dir}/desc_{idx}.npy\", descriptors)\n",
        "  # image_descriptors.append(descriptors)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ws_wq2QXVlu9"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# For cross-validation we use StratifiedKFolds because it is a variation of KFolds that keeps the proportion of each class across the sets\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "k = 3\n",
        "# 30 clusters, 23% accuracy and 17min training\n",
        "# 30 clusters akaze, 35% accuracy and 51min training\n",
        "# 30 clusters sift500, 38% accuracy and 22 min training\n",
        "accuracies = []\n",
        "\n",
        "for train_idx, val_idx in skf.split(X_train, y_train):\n",
        "  train_descriptors = [image_descriptors[i] for i in train_idx]\n",
        "  val_descriptors   = [image_descriptors[i] for i in val_idx]\n",
        "\n",
        "  # Stack all non-empty descriptors for training KMeans\n",
        "  useful_descriptors = [d for d in train_descriptors if d is not None]\n",
        "  useful_descriptors = np.vstack(useful_descriptors)\n",
        "\n",
        "  # Fit KMeans on train descriptors only\n",
        "  # After learning the kmeans we use it to label each descriptor\n",
        "  # Then we will know which cluster will have more descriptors associated\n",
        "  kmeans = KMeans(n_clusters=k, random_state=0).fit(useful_descriptors)\n",
        "\n",
        "  def compute_histograms(descriptor_list):\n",
        "        hists = []\n",
        "        for desc in descriptor_list:\n",
        "            if desc is not None:\n",
        "                # Assign each descriptor to nearest cluster center\n",
        "                cluster_labels = kmeans.predict(desc)\n",
        "                # Create histogram for this image (length = number of clusters)\n",
        "                hist, _ = np.histogram(cluster_labels, bins=np.arange(k + 1))\n",
        "            else:\n",
        "                # Image with no descriptors -> zero histogram\n",
        "                hist = np.zeros(k, dtype=int)\n",
        "            hists.append(hist)\n",
        "        return normalize(np.array(hists), norm='l2')\n",
        "\n",
        "  # BoVW histograms\n",
        "  X_tr_hist = compute_histograms(train_descriptors)\n",
        "  X_val_hist = compute_histograms(val_descriptors)\n",
        "\n",
        "  # Label\n",
        "  y_tr = [y_train[i] for i in train_idx]\n",
        "  y_val = [y_train[i] for i in val_idx]\n",
        "\n",
        "  # Train classifier\n",
        "  clf = SVC(kernel='linear')\n",
        "  clf.fit(X_tr_hist, y_tr)\n",
        "\n",
        "  # Validate\n",
        "  acc = clf.score(X_val_hist, y_val)\n",
        "  accuracies.append(acc)\n",
        "\n",
        "print(\"Mean CV accuracy:\", np.mean(accuracies))\n",
        "print(\"Std CV accuracy:\", np.std(accuracies))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67rxOtPIaS3j"
      },
      "outputs": [],
      "source": [
        "def load_desc_safe(file):\n",
        "    path = os.path.join(desc_folder, file)\n",
        "    desc = np.load(path, allow_pickle=True)\n",
        "\n",
        "    if desc is None or desc.size == 0:\n",
        "        return None\n",
        "\n",
        "    # guarantee correct shape\n",
        "    desc = np.asarray(desc, dtype=np.float32)\n",
        "\n",
        "    if desc.ndim != 2 or desc.shape[1] != 128:\n",
        "        return None\n",
        "\n",
        "    return desc\n",
        "\n",
        "desc_dir = \"descriptors\"\n",
        "\n",
        "for fname in os.listdir(desc_dir):\n",
        "    path = os.path.join(desc_dir, fname)\n",
        "\n",
        "    try:\n",
        "        desc = np.load(path, allow_pickle=True)\n",
        "    except:\n",
        "        print(\"Unreadable file:\", fname, \"— replacing with empty.\")\n",
        "        desc = None\n",
        "        np.save(path, desc)\n",
        "        continue\n",
        "\n",
        "    # Case 0: Scalar (e.g., nan)\n",
        "    if desc.ndim == 0:\n",
        "        print(\"Scalar descriptor:\", fname, \"— fixing to empty.\")\n",
        "        desc = None\n",
        "        np.save(path, desc)\n",
        "        continue\n",
        "\n",
        "    # Case 1: None or empty\n",
        "    if desc is None or desc.size == 0:\n",
        "        print(\"Fixing empty:\", fname)\n",
        "        desc = None\n",
        "        np.save(path, desc)\n",
        "        continue\n",
        "\n",
        "    # Case 2: 1D vector\n",
        "    if desc.ndim == 1:\n",
        "        if desc.shape[0] == 128:  # single descriptor\n",
        "            print(\"Fixing single vector:\", fname)\n",
        "            desc = desc.reshape(1,128)\n",
        "        else:\n",
        "            print(\"Invalid 1D descriptor:\", fname, \"— fixing to empty.\")\n",
        "            desc = None\n",
        "\n",
        "        np.save(path, desc)\n",
        "        continue\n",
        "\n",
        "    # Case 3: 2D but wrong feature length\n",
        "    if desc.ndim == 2 and desc.shape[1] != 128:\n",
        "        print(\"Invalid 2D shape:\", fname, desc.shape, \"— fixing to empty.\")\n",
        "        desc = None\n",
        "        np.save(path, desc)\n",
        "        continue\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1K0mHo5POn2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "class VLADEncoder:\n",
        "    \"\"\"\n",
        "    VLAD (Vector of Locally Aggregated Descriptors) encoder.\n",
        "    More powerful than basic BoVW histograms.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_clusters=64):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            n_clusters: Number of visual words (typically 64-256 for VLAD)\n",
        "        \"\"\"\n",
        "        self.n_clusters = n_clusters\n",
        "        self.kmeans = None\n",
        "        self.centers = None\n",
        "\n",
        "    def fit(self, descriptors):\n",
        "        \"\"\"\n",
        "        Learn visual vocabulary from descriptors.\n",
        "\n",
        "        Args:\n",
        "            descriptors: np.array of shape (n_descriptors, descriptor_dim)\n",
        "        \"\"\"\n",
        "        print(f\"Training VLAD vocabulary with {self.n_clusters} clusters...\")\n",
        "        self.kmeans = MiniBatchKMeans(\n",
        "            n_clusters=self.n_clusters,\n",
        "            random_state=42,\n",
        "            batch_size=1000,\n",
        "            max_iter=100\n",
        "        )\n",
        "        self.kmeans.fit(descriptors)\n",
        "        self.centers = self.kmeans.cluster_centers_\n",
        "        print(f\"Vocabulary trained. Centers shape: {self.centers.shape}\")\n",
        "\n",
        "    def encode_single(self, descriptors):\n",
        "        \"\"\"\n",
        "        Encode a single image's descriptors into VLAD representation.\n",
        "\n",
        "        Args:\n",
        "            descriptors: np.array of shape (n_desc, descriptor_dim)\n",
        "\n",
        "        Returns:\n",
        "            vlad: np.array of shape (n_clusters * descriptor_dim,)\n",
        "        \"\"\"\n",
        "        if descriptors is None or len(descriptors) == 0:\n",
        "            # Return zero vector for empty descriptors\n",
        "            return np.zeros(self.n_clusters * self.centers.shape[1])\n",
        "\n",
        "        # Predict cluster assignments\n",
        "        labels = self.kmeans.predict(descriptors)\n",
        "\n",
        "        # Initialize VLAD vector\n",
        "        descriptor_dim = descriptors.shape[1]\n",
        "        vlad = np.zeros((self.n_clusters, descriptor_dim))\n",
        "\n",
        "        # For each cluster, accumulate residuals\n",
        "        for cluster_idx in range(self.n_clusters):\n",
        "            # Find descriptors assigned to this cluster\n",
        "            mask = (labels == cluster_idx)\n",
        "\n",
        "            if np.sum(mask) > 0:\n",
        "                # Compute residuals: descriptor - cluster_center\n",
        "                residuals = descriptors[mask] - self.centers[cluster_idx]\n",
        "                # Sum all residuals for this cluster\n",
        "                vlad[cluster_idx] = np.sum(residuals, axis=0)\n",
        "\n",
        "        # Flatten to 1D vector\n",
        "        vlad = vlad.flatten()\n",
        "\n",
        "        # Intra-normalization: L2 normalize each cluster's residuals\n",
        "        vlad = vlad.reshape(self.n_clusters, descriptor_dim)\n",
        "        vlad = normalize(vlad, norm='l2', axis=1)\n",
        "        vlad = vlad.flatten()\n",
        "\n",
        "        # Power normalization (reduces burstiness)\n",
        "        vlad = np.sign(vlad) * np.sqrt(np.abs(vlad))\n",
        "\n",
        "        # Final L2 normalization\n",
        "        vlad = normalize(vlad.reshape(1, -1), norm='l2')[0]\n",
        "\n",
        "        return vlad\n",
        "\n",
        "    def encode_batch(self, file_list, load_desc_func):\n",
        "        \"\"\"\n",
        "        Encode multiple images.\n",
        "\n",
        "        Args:\n",
        "            file_list: List of file paths\n",
        "            load_desc_func: Function to load descriptors from file\n",
        "\n",
        "        Returns:\n",
        "            vlads: np.array of shape (n_images, n_clusters * descriptor_dim)\n",
        "        \"\"\"\n",
        "        vlads = []\n",
        "        for i, file in enumerate(file_list):\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Encoding {i}/{len(file_list)}...\")\n",
        "\n",
        "            desc = load_desc_func(file)\n",
        "            vlad = self.encode_single(desc)\n",
        "            vlads.append(vlad)\n",
        "\n",
        "        return np.array(vlads)\n",
        "\n",
        "\n",
        "# Example usage compatible with your code:\n",
        "def compute_vlad_features(file_list, load_desc_safe, n_clusters=64):\n",
        "    \"\"\"\n",
        "    Drop-in replacement for your compute_histograms function.\n",
        "\n",
        "    Args:\n",
        "        file_list: List of descriptor file paths\n",
        "        load_desc_safe: Your function to load descriptors\n",
        "        n_clusters: Number of clusters (64-256 recommended for VLAD)\n",
        "\n",
        "    Returns:\n",
        "        vlad_features: Normalized VLAD representations\n",
        "    \"\"\"\n",
        "    # Step 1: Collect all descriptors for vocabulary training\n",
        "    print(\"Collecting descriptors for vocabulary training...\")\n",
        "    all_descriptors = []\n",
        "    for file in file_list[:1000]:  # Sample for efficiency\n",
        "        desc = load_desc_safe(file)\n",
        "        if desc is not None:\n",
        "            all_descriptors.append(desc)\n",
        "\n",
        "    all_descriptors = np.vstack(all_descriptors)\n",
        "    print(f\"Collected {len(all_descriptors)} descriptors\")\n",
        "\n",
        "    # Step 2: Train VLAD encoder\n",
        "    encoder = VLADEncoder(n_clusters=n_clusters)\n",
        "    encoder.fit(all_descriptors)\n",
        "\n",
        "    # Step 3: Encode all images\n",
        "    vlad_features = encoder.encode_batch(file_list, load_desc_safe)\n",
        "\n",
        "    return vlad_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "G8KwYKx8TyPq"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "k = 256              # number of clusters\n",
        "# k=3 around 12% (same as normal kmeans)\n",
        "# k=30 around 43.5% with is actually higher than reduced sift features\n",
        "# k = 200 around 55%\n",
        "# k = 2000 around 66% but 22 min training while 200 took 5 min per cv\n",
        "# k=2000 with batch size 10k, 66% with 23 min traing\n",
        "# k=5000 10k size, 69%\n",
        "# Note that with minibatchKmeans you can increase the number of clusters without getting to penalized (in time) by it\n",
        "# They have better scaling for number of clusters than KMeans\n",
        "batch_size = 8000        # minibatch size\n",
        "desc_folder = \"descriptors\"\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "accuracies = []\n",
        "\n",
        "for train_idx, val_idx in skf.split(X_train, y_train):\n",
        "\n",
        "    # --- 1) Build list of descriptor filenames for this fold ---\n",
        "    train_files = [f\"desc_{i}.npy\" for i in train_idx]\n",
        "    val_files   = [f\"desc_{i}.npy\" for i in val_idx]\n",
        "\n",
        "    # --- 2) Train MiniBatchKMeans only on training descriptors ---\n",
        "    kmeans = MiniBatchKMeans(\n",
        "        n_clusters=k,\n",
        "        batch_size=batch_size,\n",
        "        random_state=0\n",
        "    )\n",
        "\n",
        "    batch = []\n",
        "    current_size = 0\n",
        "\n",
        "    for file in train_files:\n",
        "        desc = load_desc_safe(file)\n",
        "\n",
        "        # Ignore empty descriptors\n",
        "        if desc is None or desc.size == 0:\n",
        "          continue\n",
        "\n",
        "        # Append descriptors for streaming\n",
        "        batch.append(desc)\n",
        "        current_size += desc.shape[0]\n",
        "\n",
        "        # If enough descriptors → partial_fit\n",
        "        if current_size >= batch_size:\n",
        "            kmeans.partial_fit(np.vstack(batch))\n",
        "            batch = []\n",
        "            current_size = 0\n",
        "\n",
        "    # Fit final batch if any\n",
        "    if len(batch) > 0:\n",
        "        kmeans.partial_fit(np.vstack(batch))\n",
        "\n",
        "    # --- 3) Function to compute histograms safely ---\n",
        "    def compute_histograms(file_list):\n",
        "        hists = []\n",
        "        for file in file_list:\n",
        "            desc = load_desc_safe(file)\n",
        "            if desc is None:\n",
        "                hist = np.zeros(k, dtype=int)\n",
        "            else:\n",
        "                labels = kmeans.predict(desc)\n",
        "                hist, _ = np.histogram(labels, bins=np.arange(k+1))\n",
        "            hists.append(hist)\n",
        "        return normalize(np.array(hists), norm='l2')\n",
        "\n",
        "    # --- 4) Build BoVW histograms ---\n",
        "    # X_tr_hist = compute_histograms(train_files)\n",
        "    # X_val_hist = compute_histograms(val_files)\n",
        "\n",
        "    X_tr_hist = compute_vlad_features(train_files, load_desc_safe, n_clusters=k)\n",
        "    X_val_hist = compute_vlad_features(val_files, load_desc_safe, n_clusters=k)\n",
        "\n",
        "    # Label\n",
        "    y_tr = [y_train[i] for i in train_idx]\n",
        "    y_val = [y_train[i] for i in val_idx]\n",
        "\n",
        "    # --- 5) Train your classifier ---\n",
        "    clf = SVC(kernel='linear')\n",
        "    clf.fit(X_tr_hist, y_tr)\n",
        "\n",
        "    acc = clf.score(X_val_hist, y_val)\n",
        "    accuracies.append(acc)\n",
        "    print(\"Fold accuracy:\", acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBpWpomsaXsi"
      },
      "outputs": [],
      "source": [
        "class BagOfVisualWords:\n",
        "    def __init__(self, n_clusters=64, batch_size = 5000):\n",
        "        self.n_clusters = n_clusters\n",
        "        # By default we use MiniBatchKMeans\n",
        "        # To use standard KMeans set batch_size to -1\n",
        "        if batch_size > 0:\n",
        "            self.kmeans = MiniBatchKMeans(\n",
        "                n_clusters=n_clusters,\n",
        "                batch_size=batch_size,\n",
        "                random_state=0\n",
        "            )\n",
        "        else:\n",
        "          self.kmeans = KMeans(\n",
        "              n_clusters=n_clusters,\n",
        "              random_state=0\n",
        "          )\n",
        "        self.centers = None\n",
        "    def calculate_centers(self, descriptors):\n",
        "        self.kmeans.fit(descriptors)\n",
        "        self.centers = self.kmeans.cluster_centers_\n",
        "    def calculate_centers_batch(self, descriptors):\n",
        "        batch = []\n",
        "        current_size = 0\n",
        "        for desc in descriptors:\n",
        "            batch.append(desc)\n",
        "            current_size += desc.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLNXOgNbvuwB"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "clf = GradientBoostingClassifier(n_estimators=100,random_state=0)\n",
        "clf.fit(X_tr_hist, y_tr)\n",
        "\n",
        "acc = clf.score(X_val_hist, y_val)\n",
        "print(\"Fold accuracy:\", acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9liWx0nT0jg"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier(n_estimators=150, random_state=42)\n",
        "classifier.fit(X_tr_hist, y_tr)\n",
        "acc = clf.score(X_val_hist, y_val)\n",
        "print(\"Fold accuracy:\", acc)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
