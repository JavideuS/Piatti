{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GcuNuqTXYZDI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hola mundo\n"
          ]
        }
      ],
      "source": [
        "# Assigment 1: Comparative Study of Classification Algorithms.\n",
        "print(\"Hola mundo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mQd5APwtaJgd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mundo dice ciao\n",
            "Ciao dice mundo\n",
            "Mundo dice que ciao lava piati\n"
          ]
        }
      ],
      "source": [
        "print(\"Mundo dice ciao\")\n",
        "print(\"Ciao dice mundo\")\n",
        "print(\"Mundo dice que ciao lava piati\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PL71bsABbaqg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mundo gioca calcio\n"
          ]
        }
      ],
      "source": [
        "print(\"Mundo gioca calcio\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-E61_93x60v2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Giusseppe se puso la peluca\n"
          ]
        }
      ],
      "source": [
        "print(\"Giusseppe se puso la peluca\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDE8NrPXKdZ2",
        "outputId": "74992e8f-f016-4216-b116-045c3334f8e6"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'kagglehub'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkagglehub\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshutil\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'kagglehub'"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Dataset directory\n",
        "dataset_dir = \"data/raw\"\n",
        "\n",
        "# Download dataset\n",
        "path = kagglehub.dataset_download(\"jiayuanchengala/aid-scene-classification-datasets\")\n",
        "src_dir = os.path.join(path, \"AID\")\n",
        "\n",
        "# Move only if not already moved\n",
        "if not os.path.exists(dataset_dir):\n",
        "    print(\"Copying dataset to /content/data/raw...\")\n",
        "    # Note that if I move it then kagglehub wont re-download so we can=t keep consistency\n",
        "    shutil.copytree(src_dir, dataset_dir)\n",
        "else:\n",
        "    print(\"Dataset already exists at /content/data/raw\")\n",
        "\n",
        "print(\"✅ Dataset ready at:\", dataset_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOrWQiOhTJMZ"
      },
      "source": [
        "**Manual Data Separation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hueuKpDe6uw9",
        "outputId": "782a8584-70e0-4182-d8c9-acb4d5c25d18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 10000 images in 30 categories\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "image_paths =  []\n",
        "labels = []\n",
        "source_dir = 'data/raw'\n",
        "categories = [d for d in os.listdir(source_dir)\n",
        "              if os.path.isdir(os.path.join(source_dir, d))]\n",
        "for category in categories:\n",
        "    category_path = os.path.join(source_dir, category)\n",
        "    files = [os.path.join(category_path, f) for f in os.listdir(category_path)\n",
        "             if os.path.isfile(os.path.join(category_path, f))]\n",
        "    image_paths.extend(files)\n",
        "    labels.extend([category] * len(files))\n",
        "\n",
        "print(f\"Found {len(image_paths)} images in {len(categories)} categories\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCZYPpJOU1kV",
        "outputId": "b6bbe54c-83ca-432c-cd48-f6e95b34ec56"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Source directory containing all the category folders\n",
        "source_dir = 'data/raw'\n",
        "\n",
        "# Create train and test directories\n",
        "train_dir = 'data/train'\n",
        "test_dir = 'data/test'\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "for category in categories:\n",
        "    category_path = os.path.join(source_dir, category)\n",
        "\n",
        "    if not os.path.exists(category_path):\n",
        "        print(f\"Warning: {category} folder not found, skipping...\")\n",
        "        continue\n",
        "\n",
        "    # Get all files in the category folder\n",
        "    files = [f for f in os.listdir(category_path)\n",
        "             if os.path.isfile(os.path.join(category_path, f))]\n",
        "\n",
        "    if len(files) == 0:\n",
        "        print(f\"Warning: {category} folder is empty, skipping...\")\n",
        "        continue\n",
        "\n",
        "    # Shuffle files randomly\n",
        "    random.shuffle(files)\n",
        "\n",
        "    # Calculate split point (90% for train)\n",
        "    split_idx = int(len(files) * 0.9)\n",
        "    train_files = files[:split_idx]\n",
        "    test_files = files[split_idx:]\n",
        "\n",
        "    # Create category subfolders in train and test\n",
        "    train_category_dir = os.path.join(train_dir, category)\n",
        "    test_category_dir = os.path.join(test_dir, category)\n",
        "    os.makedirs(train_category_dir, exist_ok=True)\n",
        "    os.makedirs(test_category_dir, exist_ok=True)\n",
        "\n",
        "    # Move files to train\n",
        "    for file in train_files:\n",
        "        src = os.path.join(category_path, file)\n",
        "        dst = os.path.join(train_category_dir, file)\n",
        "        shutil.copy2(src, dst)\n",
        "\n",
        "    # Move files to test\n",
        "    for file in test_files:\n",
        "        src = os.path.join(category_path, file)\n",
        "        dst = os.path.join(test_category_dir, file)\n",
        "        shutil.copy2(src, dst)\n",
        "\n",
        "    print(f\"{category}: {len(train_files)} files to train, {len(test_files)} files to test\")\n",
        "\n",
        "print(\"\\nSplit complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GCdL4xacn_y",
        "outputId": "7359f608-42b2-48bc-de16-f874f6dd0052"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# This is for cross-validation (Pytorch and Tensorflow need manual separation)\n",
        "# To scikit-learning you onl need to pass the training data and it does the division\n",
        "\n",
        "# Configuration\n",
        "k_folds = 5\n",
        "output_base = 'kfolds'\n",
        "os.makedirs(output_base, exist_ok=True)\n",
        "\n",
        "print(f\"Creating {k_folds}-fold cross-validation split...\")\n",
        "\n",
        "# Collect all files by category\n",
        "category_files = {}\n",
        "for category in categories:\n",
        "    category_path = os.path.join(source_dir, category)\n",
        "    files = [f for f in os.listdir(category_path)\n",
        "             if os.path.isfile(os.path.join(category_path, f))]\n",
        "    random.shuffle(files)  # Shuffle within each category\n",
        "    category_files[category] = files\n",
        "\n",
        "# Create fold assignments for each category\n",
        "fold_assignments = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "for category, files in category_files.items():\n",
        "    n_files = len(files)\n",
        "    fold_size = n_files // k_folds\n",
        "\n",
        "    # Assign files to folds ensuring balanced distribution\n",
        "    for fold_idx in range(k_folds):\n",
        "        start_idx = fold_idx * fold_size\n",
        "        # Last fold gets any remaining files\n",
        "        end_idx = start_idx + fold_size if fold_idx < k_folds - 1 else n_files\n",
        "        fold_assignments[fold_idx][category] = files[start_idx:end_idx]\n",
        "\n",
        "# Create fold directories with class subdirectories only\n",
        "for fold_idx in range(k_folds):\n",
        "    fold_name = f'fold_{fold_idx + 1}'\n",
        "    fold_dir = os.path.join(output_base, fold_name)\n",
        "\n",
        "    # Create class subdirectories in each fold\n",
        "    for category in categories:\n",
        "        category_fold_dir = os.path.join(fold_dir, category)\n",
        "        os.makedirs(category_fold_dir, exist_ok=True)\n",
        "\n",
        "        # Copy the files assigned to this fold for this category\n",
        "        files_for_this_fold = fold_assignments[fold_idx][category]\n",
        "\n",
        "        for file in files_for_this_fold:\n",
        "            src = os.path.join(source_dir, category, file)\n",
        "            dst = os.path.join(category_fold_dir, file)\n",
        "            shutil.copy2(src, dst)\n",
        "\n",
        "    print(f\"\\n{fold_name}:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    total_files = 0\n",
        "    for category in categories:\n",
        "        files_in_fold = fold_assignments[fold_idx][category]\n",
        "        total_files += len(files_in_fold)\n",
        "        print(f\"  {category}: {len(files_in_fold)} files\")\n",
        "\n",
        "    print(f\"  Total in fold: {total_files} files\")\n",
        "\n",
        "print(f\"\\n\\nK-fold split complete!\")\n",
        "print(f\"Output directory: {output_base}/\")\n",
        "print(f\"Structure: fold_1/, fold_2/, ..., fold_{k_folds}/\")\n",
        "print(f\"Each fold contains: {', '.join(categories)} subdirectories with their respective files\")\n",
        "\n",
        "print(f\"\\n\\nK-fold split complete!\")\n",
        "print(f\"Output directory: {output_base}/\")\n",
        "print(f\"Structure: fold_1/, fold_2/, ..., fold_{k_folds}/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvDO7tPCTiTM"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEk296AuPYxq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Corrupt JPEG data: premature end of data segment\n"
          ]
        }
      ],
      "source": [
        "# Since scikit-learning doesn't have features extraction for images\n",
        "# We have to use also openCV to create our dictionary of features\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(image_paths, labels, test_size=0.2, random_state=0, stratify=labels)\n",
        "\n",
        "# We get all descriptors for training set before kfold, does not count as data leakage\n",
        "# It just avoids calculating them k times\n",
        "orb = cv.ORB_create() # Fast but poor performance\n",
        "# sift = cv.SIFT_create(nfeatures=2000) #13gb is not enough for sift(40+ min)\n",
        "sift = cv.SIFT_create()\n",
        "# surf = cv.xfeatures2d.SURF_create() Non-free\n",
        "akaze = cv.AKAZE_create(descriptor_type=cv.AKAZE_DESCRIPTOR_MLDB,\n",
        "                         descriptor_size=0,   # 486 bits\n",
        "                         threshold=0.001) #Slightly faster same results\n",
        "fast = cv.FastFeatureDetector_create()\n",
        "\n",
        "#Sift with 500 features performs a faster than akaze and just has 2% less accuracy (10 min training and faster kmeans)\n",
        "# 2000 features takes 8gb rams and takes around 35-40min, but can't do kmeans)\n",
        "# 1500 features takes 6.7 and around 35 min, but still isnt enough for kmeans\n",
        "# 1000 features takes 5gb ram and around 31 min, but still doesnt work with kmeans\n",
        "# Kmeans consumes around 6gb ram, so any detector method that keeps more than 6gb too will probably crash\n",
        "\n",
        "# Note that this also affect the kmeans\n",
        "# Seems like more computing time on getting labels is also translated to longer kmean\n",
        "\n",
        "image_descriptors = []\n",
        "pca = PCA(n_components=64) # Reduce descriptor dimension by half from 128 to 64\n",
        "output_dir = \"descriptors\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for idx, x in enumerate(X_train):\n",
        "  # print(x, type(x))\n",
        "  p = Path(x)\n",
        "  # print(p.stem, type(p.stem))\n",
        "  img = cv.imread(x, cv.IMREAD_GRAYSCALE)\n",
        "  keypoints = sift.detect(img, None)\n",
        "  keypoints, descriptors = sift.compute(img, keypoints)\n",
        "  # Reducing precision from float32 to float16\n",
        "  # if descriptors is not None:\n",
        "    # descriptors = descriptors.astype('float16')\n",
        "    # descriptors = pca.fit_transform(descriptors)\n",
        "  np.save(f\"{output_dir}/{p.stem}.npy\", descriptors)\n",
        "  image_descriptors.append(descriptors)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67rxOtPIaS3j"
      },
      "outputs": [],
      "source": [
        "def load_desc_safe(file):\n",
        "    path = os.path.join(desc_folder, file)\n",
        "    desc = np.load(path, allow_pickle=True)\n",
        "\n",
        "    if desc is None or desc.size == 0:\n",
        "        return None\n",
        "\n",
        "    # guarantee correct shape\n",
        "    desc = np.asarray(desc, dtype=np.float32)\n",
        "\n",
        "    if desc.ndim != 2 or desc.shape[1] != 128:\n",
        "        return None\n",
        "\n",
        "    return desc\n",
        "\n",
        "# desc_dir = \"descriptors\"\n",
        "\n",
        "# for fname in os.listdir(desc_dir):\n",
        "#     path = os.path.join(desc_dir, fname)\n",
        "\n",
        "#     try:\n",
        "#         desc = np.load(path, allow_pickle=True)\n",
        "#     except:\n",
        "#         print(\"Unreadable file:\", fname, \"— replacing with empty.\")\n",
        "#         desc = None\n",
        "#         np.save(path, desc)\n",
        "#         continue\n",
        "\n",
        "#     # Case 0: Scalar (e.g., nan)\n",
        "#     if desc.ndim == 0:\n",
        "#         print(\"Scalar descriptor:\", fname, \"— fixing to empty.\")\n",
        "#         desc = None\n",
        "#         np.save(path, desc)\n",
        "#         continue\n",
        "\n",
        "#     # Case 1: None or empty\n",
        "#     if desc is None or desc.size == 0:\n",
        "#         print(\"Fixing empty:\", fname)\n",
        "#         desc = None\n",
        "#         np.save(path, desc)\n",
        "#         continue\n",
        "\n",
        "#     # Case 2: 1D vector\n",
        "#     if desc.ndim == 1:\n",
        "#         if desc.shape[0] == 128:  # single descriptor\n",
        "#             print(\"Fixing single vector:\", fname)\n",
        "#             desc = desc.reshape(1,128)\n",
        "#         else:\n",
        "#             print(\"Invalid 1D descriptor:\", fname, \"— fixing to empty.\")\n",
        "#             desc = None\n",
        "\n",
        "#         np.save(path, desc)\n",
        "#         continue\n",
        "\n",
        "#     # Case 3: 2D but wrong feature length\n",
        "#     if desc.ndim == 2 and desc.shape[1] != 128:\n",
        "#         print(\"Invalid 2D shape:\", fname, desc.shape, \"— fixing to empty.\")\n",
        "#         desc = None\n",
        "#         np.save(path, desc)\n",
        "#         continue\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9Z3nXepIqTA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
        "from sklearn.preprocessing import normalize\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import List, Optional, Union\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def descriptor_batch_generator(data_loader, load_func=None, max_descriptors=10000):\n",
        "    \"\"\"\n",
        "    Yields batches of descriptors with a total number of rows up to max_descriptors.\n",
        "\n",
        "    Args:\n",
        "        data_loader: iterable of items to load descriptors from.\n",
        "        load_func: optional function(item) -> descriptor array\n",
        "        max_descriptors: max total rows per batch\n",
        "\n",
        "    Yields:\n",
        "        np.ndarray of shape (~max_descriptors, n_features)\n",
        "    \"\"\"\n",
        "    batch = []\n",
        "    current_size = 0   # number of descriptors accumulated\n",
        "\n",
        "    for item in data_loader:\n",
        "        data = load_func(item) if load_func else item\n",
        "\n",
        "        # Skip empty or None\n",
        "        if data is None or data.size == 0:\n",
        "            continue\n",
        "\n",
        "        # Ensure 2D\n",
        "        if data.ndim == 1:\n",
        "            data = data.reshape(1, -1)\n",
        "\n",
        "        batch.append(data)\n",
        "        current_size += data.shape[0]\n",
        "\n",
        "        # If batch is full, yield it\n",
        "        if current_size >= max_descriptors:\n",
        "            yield np.vstack(batch)\n",
        "            batch = []\n",
        "            current_size = 0\n",
        "\n",
        "    # Yield remainder\n",
        "    if batch:\n",
        "        yield np.vstack(batch)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Base Classes\n",
        "# ============================================================================\n",
        "\n",
        "class ClusteringAlgorithm(ABC):\n",
        "    \"\"\"Base class for clustering algorithms.\"\"\"\n",
        "\n",
        "    def __init__(self, n_clusters: int, random_state: int = 42):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.random_state = random_state\n",
        "        self.model = None\n",
        "        self._is_fitted = False\n",
        "\n",
        "    @abstractmethod\n",
        "    def fit(self, X: np.ndarray):\n",
        "        \"\"\"Fit the clustering model.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def fit_iterative(self, data_loader, load_func: Optional[callable] = None):\n",
        "        \"\"\"Fit iteratively for large datasets.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Predict cluster assignments.\"\"\"\n",
        "        if not self._is_fitted:\n",
        "            raise ValueError(\"Model must be fitted before predicting.\")\n",
        "        return self.model.predict(X)\n",
        "\n",
        "    @property\n",
        "    def cluster_centers_(self) -> np.ndarray:\n",
        "        \"\"\"Get cluster centers.\"\"\"\n",
        "        if not self._is_fitted:\n",
        "            raise ValueError(\"Model must be fitted first.\")\n",
        "        return self.model.cluster_centers_\n",
        "\n",
        "\n",
        "class EncodingAlgorithm(ABC):\n",
        "    \"\"\"Base class for encoding algorithms (BoVW, VLAD, Fisher).\"\"\"\n",
        "\n",
        "    def __init__(self, clustering: ClusteringAlgorithm):\n",
        "        self.clustering = clustering\n",
        "\n",
        "    @abstractmethod\n",
        "    def encode(self, descriptors: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Encode descriptors into a fixed-length vector.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def encode_batch(self, descriptors_list: List[np.ndarray]) -> np.ndarray:\n",
        "        \"\"\"Encode a batch of descriptor sets.\"\"\"\n",
        "        pass\n",
        "\n",
        "# ============================================================================\n",
        "# Clustering Implementations\n",
        "# ============================================================================\n",
        "\n",
        "class KMeansClustering(ClusteringAlgorithm):\n",
        "    \"\"\"Standard KMeans clustering.\"\"\"\n",
        "\n",
        "    def __init__(self, n_clusters: int, random_state: int = 0, **kwargs):\n",
        "        super().__init__(n_clusters, random_state)\n",
        "        self.kwargs = kwargs\n",
        "        self.model = KMeans(\n",
        "            n_clusters=n_clusters,\n",
        "            random_state=random_state,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    def fit(self, X: np.ndarray):\n",
        "        \"\"\"Fit KMeans on all data at once.\"\"\"\n",
        "        self.model.fit(X)\n",
        "        self._is_fitted = True\n",
        "        return self\n",
        "\n",
        "    def fit_iterative(self, data_loader, load_func: Optional[callable] = None):\n",
        "        \"\"\"KMeans doesn't support iterative fitting - loads all data first.\"\"\"\n",
        "        print(\"Warning: KMeans doesn't support true iterative fitting. Loading all data...\")\n",
        "\n",
        "        all_data = []\n",
        "        for item in data_loader:\n",
        "            data = load_func(item) if load_func else item\n",
        "            if data is not None and data.size > 0:\n",
        "                all_data.append(data)\n",
        "\n",
        "        if all_data:\n",
        "            X = np.vstack(all_data)\n",
        "            self.fit(X)\n",
        "        return self\n",
        "\n",
        "\n",
        "class MiniBatchKMeansClustering(ClusteringAlgorithm):\n",
        "    \"\"\"MiniBatch KMeans with support for iterative fitting.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_clusters: int,\n",
        "        batch_size: int = 1024,\n",
        "        random_state: int = 0,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(n_clusters, random_state)\n",
        "        self.batch_size = batch_size\n",
        "        self.kwargs = kwargs\n",
        "        self.model = MiniBatchKMeans(\n",
        "            n_clusters=n_clusters,\n",
        "            batch_size=batch_size,\n",
        "            random_state=random_state,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    def fit(self, X: np.ndarray):\n",
        "        \"\"\"Fit MiniBatchKMeans on all data at once.\"\"\"\n",
        "        self.model.fit(X)\n",
        "        self._is_fitted = True\n",
        "        return self\n",
        "\n",
        "    def fit_iterative(\n",
        "        self,\n",
        "        data_loader,\n",
        "        load_func: Optional[callable] = None,\n",
        "        accumulate_batch_size: Optional[int] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Fit iteratively using partial_fit.\n",
        "\n",
        "        Args:\n",
        "            data_loader: Iterable of file paths or data chunks\n",
        "            load_func: Function to load data from paths\n",
        "            accumulate_batch_size: Size to accumulate before partial_fit\n",
        "                                   (defaults to self.batch_size)\n",
        "        \"\"\"\n",
        "        if accumulate_batch_size is None:\n",
        "            accumulate_batch_size = self.batch_size\n",
        "\n",
        "        for batch in descriptor_batch_generator(data_loader, load_func, accumulate_batch_size):\n",
        "            self.model.partial_fit(batch)\n",
        "\n",
        "        self._is_fitted = True\n",
        "        return self\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Encoding Implementations\n",
        "# ============================================================================\n",
        "\n",
        "class BagOfVisualWords(EncodingAlgorithm):\n",
        "    \"\"\"\n",
        "    Bag of Visual Words encoding.\n",
        "    Represents an image as a histogram of visual word occurrences.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, clustering: ClusteringAlgorithm, normalize: bool = True):\n",
        "        super().__init__(clustering)\n",
        "        self.normalize = normalize\n",
        "\n",
        "    def encode(self, descriptors: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Encode descriptors into BoVW histogram.\n",
        "\n",
        "        Args:\n",
        "            descriptors: Array of shape (n_descriptors, n_features)\n",
        "\n",
        "        Returns:\n",
        "            Histogram of visual words of shape (n_clusters,)\n",
        "        \"\"\"\n",
        "        if descriptors is None or descriptors.size == 0:\n",
        "            return np.zeros(self.clustering.n_clusters)\n",
        "\n",
        "        # Assign each descriptor to nearest cluster\n",
        "        labels = self.clustering.predict(descriptors)\n",
        "\n",
        "        # Build histogram\n",
        "        histogram = np.bincount(\n",
        "            labels,\n",
        "            minlength=self.clustering.n_clusters\n",
        "        ).astype(np.float32)\n",
        "\n",
        "\n",
        "        # L2 normalization\n",
        "        norm = np.linalg.norm(histogram)\n",
        "        if norm > 0:\n",
        "            histogram = histogram / norm\n",
        "\n",
        "        # Normalize l1\n",
        "        # if self.normalize and histogram.sum() > 0:\n",
        "        #     histogram /= histogram.sum()\n",
        "\n",
        "        return histogram\n",
        "\n",
        "    def encode_batch(self, descriptors_list: List[np.ndarray]) -> np.ndarray:\n",
        "        \"\"\"Encode multiple descriptor sets.\"\"\"\n",
        "        return np.array([self.encode(desc) for desc in descriptors_list])\n",
        "\n",
        "\n",
        "class VLAD(EncodingAlgorithm):\n",
        "    \"\"\"\n",
        "    Vector of Locally Aggregated Descriptors.\n",
        "    Accumulates residuals between descriptors and cluster centers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, clustering: ClusteringAlgorithm, normalize: bool = True):\n",
        "        super().__init__(clustering)\n",
        "        self.normalize = normalize\n",
        "\n",
        "    def encode(self, descriptors: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Encode descriptors (of a single image) into VLAD vector.\n",
        "\n",
        "        Args:\n",
        "            descriptors: Array of shape (n_descriptors, n_features)\n",
        "\n",
        "        Returns:\n",
        "            VLAD vector of shape (n_clusters * n_features,)\n",
        "        \"\"\"\n",
        "        if descriptors is None or descriptors.size == 0:\n",
        "            n_features = self.clustering.cluster_centers_.shape[1]\n",
        "            return np.zeros(self.clustering.n_clusters * n_features)\n",
        "\n",
        "        # Assign descriptors to clusters\n",
        "        labels = self.clustering.predict(descriptors)\n",
        "        centers = self.clustering.cluster_centers_\n",
        "\n",
        "        # Compute VLAD\n",
        "        n_clusters, n_features = centers.shape\n",
        "        vlad = np.zeros((n_clusters, n_features))\n",
        "\n",
        "        for idx in range(n_clusters):\n",
        "            # Find descriptors assigned to cluster i\n",
        "            mask = (labels == idx)\n",
        "            if np.any(mask):\n",
        "                # Sum of residuals\n",
        "                residuals = descriptors[mask] - centers[idx]\n",
        "                vlad[idx] = residuals.sum(axis=0)\n",
        "\n",
        "        # Flatten and normalize\n",
        "        vlad = vlad.flatten()\n",
        "\n",
        "        # Intra-normalization: L2 normalize each cluster's residuals\n",
        "        vlad = vlad.reshape(n_clusters, n_features)\n",
        "        vlad = normalize(vlad, norm='l2', axis=1)\n",
        "        vlad = vlad.flatten()\n",
        "\n",
        "        # Power normalization (reduces burstiness)\n",
        "        vlad = np.sign(vlad) * np.sqrt(np.abs(vlad))\n",
        "\n",
        "        if self.normalize:\n",
        "            # L2 normalization (per image)\n",
        "            vlad = normalize(vlad.reshape(1, -1), norm='l2').flatten()\n",
        "\n",
        "        return vlad\n",
        "\n",
        "    def encode_batch(self, image_list: List[np.ndarray], load_function) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Encode a batch of batch of images (descriptors) into VLAD vectors.\n",
        "\n",
        "        Args:\n",
        "            descriptors_list: List of each image descriptors,\n",
        "                            each of shape (n_descriptors, n_features)\n",
        "\n",
        "        Returns:\n",
        "            Array of VLAD vectors of shape\n",
        "            (batch_size, n_clusters * n_features)\n",
        "        \"\"\"\n",
        "        vlad_vectors = []\n",
        "\n",
        "        for item in image_list:\n",
        "            descriptors = load_function(item)\n",
        "            vlad = self.encode(descriptors)\n",
        "            vlad_vectors.append(vlad)\n",
        "\n",
        "        return np.vstack(vlad_vectors)\n",
        "\n",
        "\n",
        "\n",
        "class FisherVector(EncodingAlgorithm):\n",
        "    \"\"\"\n",
        "    Fisher Vector encoding.\n",
        "    Note: This is a simplified implementation. For production use,\n",
        "    consider using a GMM-based approach.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, clustering: ClusteringAlgorithm, normalize: bool = True):\n",
        "        super().__init__(clustering)\n",
        "        self.normalize = normalize\n",
        "        print(\"Note: This is a simplified Fisher Vector using K-means.\")\n",
        "        print(\"For full Fisher Vectors, use sklearn.mixture.GaussianMixture\")\n",
        "\n",
        "    def encode(self, descriptors: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Simplified Fisher Vector encoding using K-means centers.\n",
        "\n",
        "        Returns:\n",
        "            Fisher vector of shape (n_clusters * n_features * 2,)\n",
        "        \"\"\"\n",
        "        if descriptors is None or descriptors.size == 0:\n",
        "            n_features = self.clustering.cluster_centers_.shape[1]\n",
        "            return np.zeros(self.clustering.n_clusters * n_features * 2)\n",
        "\n",
        "        labels = self.clustering.predict(descriptors)\n",
        "        centers = self.clustering.cluster_centers_\n",
        "\n",
        "        n_clusters, n_features = centers.shape\n",
        "        fv = np.zeros((n_clusters, n_features, 2), dtype=np.float32)\n",
        "\n",
        "        for i in range(n_clusters):\n",
        "            mask = labels == i\n",
        "            if np.any(mask):\n",
        "                cluster_desc = descriptors[mask]\n",
        "                n_k = cluster_desc.shape[0]\n",
        "\n",
        "                # Mean deviation\n",
        "                mean_dev = (cluster_desc - centers[i]).sum(axis=0) / np.sqrt(n_k)\n",
        "                fv[i, :, 0] = mean_dev\n",
        "\n",
        "                # Variance deviation (simplified)\n",
        "                var_dev = ((cluster_desc - centers[i]) ** 2).sum(axis=0) / np.sqrt(n_k)\n",
        "                fv[i, :, 1] = var_dev - 1.0\n",
        "\n",
        "        fv = fv.flatten()\n",
        "\n",
        "        if self.normalize:\n",
        "            norm = np.linalg.norm(fv)\n",
        "            if norm > 0:\n",
        "                fv /= norm\n",
        "\n",
        "        return fv\n",
        "\n",
        "    def encode_batch(self, descriptors_list: List[np.ndarray]) -> np.ndarray:\n",
        "        \"\"\"Encode multiple descriptor sets.\"\"\"\n",
        "        return np.array([self.encode(desc) for desc in descriptors_list])\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# High-Level Pipeline\n",
        "# ============================================================================\n",
        "\n",
        "class VisualEncodingPipeline:\n",
        "    \"\"\"\n",
        "    High-level pipeline combining clustering and encoding.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        clustering: ClusteringAlgorithm,\n",
        "        encoding: EncodingAlgorithm\n",
        "    ):\n",
        "        self.clustering = clustering\n",
        "        self.encoding = encoding\n",
        "\n",
        "    def fit(self, descriptors: Union[np.ndarray, List], load_func: Optional[callable] = None):\n",
        "        \"\"\"\n",
        "        Fit the clustering model.\n",
        "\n",
        "        Args:\n",
        "            descriptors: Either:\n",
        "                - np.ndarray: All descriptors stacked\n",
        "                - List: List of file paths or data chunks for iterative fitting\n",
        "            load_func: Function to load descriptors from paths (for iterative fitting)\n",
        "        \"\"\"\n",
        "        if isinstance(descriptors, np.ndarray):\n",
        "            self.clustering.fit(descriptors)\n",
        "        else:\n",
        "            self.clustering.fit_iterative(descriptors, load_func)\n",
        "        return self\n",
        "\n",
        "    def transform(self, descriptors: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Encode a single set of descriptors.\"\"\"\n",
        "        return self.encoding.encode(descriptors)\n",
        "\n",
        "    def fit_transform(self, train_descriptors, transform_descriptors: np.ndarray):\n",
        "        \"\"\"Fit on training data and transform other data.\"\"\"\n",
        "        self.fit(train_descriptors)\n",
        "        return self.transform(transform_descriptors)\n",
        "\n",
        "    def transform_batch(self, descriptors_list: List[np.ndarray]) -> np.ndarray:\n",
        "        \"\"\"Encode multiple sets of descriptors.\"\"\"\n",
        "        return self.encoding.encode_batch(descriptors_list)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Example Usage\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example 1: BoVW with MiniBatchKMeans (iterative fitting)\n",
        "    print(\"=== Example 1: BoVW with MiniBatch KMeans ===\")\n",
        "\n",
        "    # Create clustering\n",
        "    clustering = MiniBatchKMeansClustering(n_clusters=64, batch_size=1024)\n",
        "\n",
        "    # Create encoding\n",
        "    bovw = BagOfVisualWords(clustering, normalize=True)\n",
        "\n",
        "    # Create pipeline\n",
        "    pipeline = VisualEncodingPipeline(clustering, bovw)\n",
        "\n",
        "    # Simulate file loading\n",
        "    def load_descriptors(filepath):\n",
        "        return np.random.rand(100, 128).astype(np.float32)\n",
        "\n",
        "    train_files = [f\"train_{i}.npy\" for i in range(50)]\n",
        "\n",
        "    # Fit iteratively\n",
        "    pipeline.fit(train_files, load_func=load_descriptors)\n",
        "\n",
        "    # Encode a single image\n",
        "    test_desc = np.random.rand(80, 128).astype(np.float32)\n",
        "    bovw_vector = pipeline.transform(test_desc)\n",
        "    print(f\"BoVW vector shape: {bovw_vector.shape}\")\n",
        "    print(f\"BoVW vector sum: {bovw_vector.sum():.3f}\")\n",
        "\n",
        "    # Example 2: VLAD with standard KMeans\n",
        "    print(\"\\n=== Example 2: VLAD with KMeans ===\")\n",
        "\n",
        "    # All data fits in memory\n",
        "    all_train_data = np.random.rand(5000, 128).astype(np.float32)\n",
        "\n",
        "    clustering2 = KMeansClustering(n_clusters=32)\n",
        "    vlad = VLAD(clustering2, normalize=True)\n",
        "    pipeline2 = VisualEncodingPipeline(clustering2, vlad)\n",
        "\n",
        "    pipeline2.fit(all_train_data)\n",
        "    vlad_vector = pipeline2.transform(test_desc)\n",
        "    print(f\"VLAD vector shape: {vlad_vector.shape}\")\n",
        "\n",
        "    # Example 3: Batch encoding\n",
        "    print(\"\\n=== Example 3: Batch encoding ===\")\n",
        "\n",
        "    test_images = [np.random.rand(80, 128).astype(np.float32) for _ in range(5)]\n",
        "    encoded_batch = pipeline.transform_batch(test_images)\n",
        "    print(f\"Batch encoded shape: {encoded_batch.shape}\")\n",
        "\n",
        "    # Example 4: Compare different encodings\n",
        "    print(\"\\n=== Example 4: Different encodings comparison ===\")\n",
        "\n",
        "    clustering3 = MiniBatchKMeansClustering(n_clusters=64)\n",
        "    clustering3.fit(all_train_data)\n",
        "\n",
        "    encodings = {\n",
        "        \"BoVW\": BagOfVisualWords(clustering3),\n",
        "        \"VLAD\": VLAD(clustering3),\n",
        "        \"Fisher\": FisherVector(clustering3)\n",
        "    }\n",
        "\n",
        "    for name, enc in encodings.items():\n",
        "        vector = enc.encode(test_desc)\n",
        "        print(f\"{name:10s} - Shape: {vector.shape}, Norm: {np.linalg.norm(vector):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEmQ-cneJLDg"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "k = 256              # number of clusters\n",
        "# k=3 around 12% (same as normal kmeans)\n",
        "# k=30 around 43.5% with is actually higher than reduced sift features\n",
        "# k = 200 around 55%\n",
        "# k = 2000 around 66% but 22 min training while 200 took 5 min per cv\n",
        "# k=2000 with batch size 10k, 66% with 23 min traing\n",
        "# k=5000 10k size, 69%\n",
        "# Note that with minibatchKmeans you can increase the number of clusters without getting to penalized (in time) by it\n",
        "# They have better scaling for number of clusters than KMeans\n",
        "# VLAD -> Faster clustering but longer training 72% precision with k=256 around 1hour\n",
        "batch_size = 8000        # minibatch size\n",
        "desc_folder = \"descriptors\"\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "accuracies = []\n",
        "\n",
        "for train_idx, val_idx in skf.split(X_train, y_train):\n",
        "\n",
        "    # --- 1) Build list of descriptor filenames for this fold ---\n",
        "    train_files = [f\"desc_{i}.npy\" for i in train_idx]\n",
        "    val_files   = [f\"desc_{i}.npy\" for i in val_idx]\n",
        "\n",
        "    # --- 2) Train MiniBatchKMeans only on training descriptors ---\n",
        "    clustering = MiniBatchKMeansClustering(n_clusters=256, batch_size=4096)\n",
        "    clustering.fit_iterative(train_files, load_desc_safe)\n",
        "\n",
        "    vlad = VLAD(clustering)\n",
        "    X_tr_hist = vlad.encode_batch(train_files, load_desc_safe)\n",
        "    X_val_hist = vlad.encode_batch(val_files, load_desc_safe)\n",
        "\n",
        "    print(\"Ready to train\")\n",
        "\n",
        "    # Label\n",
        "    y_tr = [y_train[i] for i in train_idx]\n",
        "    y_val = [y_train[i] for i in val_idx]\n",
        "\n",
        "    # --- 5) Train your classifier ---\n",
        "    clf = SVC(kernel='linear')\n",
        "    clf.fit(X_tr_hist, y_tr)\n",
        "\n",
        "    acc = clf.score(X_val_hist, y_val)\n",
        "    accuracies.append(acc)\n",
        "    print(\"Fold accuracy:\", acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ws_wq2QXVlu9"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# For cross-validation we use StratifiedKFolds because it is a variation of KFolds that keeps the proportion of each class across the sets\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "k = 3\n",
        "# 30 clusters, 23% accuracy and 17min training\n",
        "# 30 clusters akaze, 35% accuracy and 51min training\n",
        "# 30 clusters sift500, 38% accuracy and 22 min training\n",
        "accuracies = []\n",
        "\n",
        "for train_idx, val_idx in skf.split(X_train, y_train):\n",
        "  train_descriptors = [image_descriptors[i] for i in train_idx]\n",
        "  val_descriptors   = [image_descriptors[i] for i in val_idx]\n",
        "\n",
        "  # Stack all non-empty descriptors for training KMeans\n",
        "  useful_descriptors = [d for d in train_descriptors if d is not None]\n",
        "  useful_descriptors = np.vstack(useful_descriptors)\n",
        "\n",
        "  # Fit KMeans on train descriptors only\n",
        "  # After learning the kmeans we use it to label each descriptor\n",
        "  # Then we will know which cluster will have more descriptors associated\n",
        "  kmeans = KMeans(n_clusters=k, random_state=0).fit(useful_descriptors)\n",
        "\n",
        "  def compute_histograms(descriptor_list):\n",
        "        hists = []\n",
        "        for desc in descriptor_list:\n",
        "            if desc is not None:\n",
        "                # Assign each descriptor to nearest cluster center\n",
        "                cluster_labels = kmeans.predict(desc)\n",
        "                # Create histogram for this image (length = number of clusters)\n",
        "                hist, _ = np.histogram(cluster_labels, bins=np.arange(k + 1))\n",
        "            else:\n",
        "                # Image with no descriptors -> zero histogram\n",
        "                hist = np.zeros(k, dtype=int)\n",
        "            hists.append(hist)\n",
        "        return normalize(np.array(hists), norm='l2')\n",
        "\n",
        "  # BoVW histograms\n",
        "  X_tr_hist = compute_histograms(train_descriptors)\n",
        "  X_val_hist = compute_histograms(val_descriptors)\n",
        "\n",
        "  # Label\n",
        "  y_tr = [y_train[i] for i in train_idx]\n",
        "  y_val = [y_train[i] for i in val_idx]\n",
        "\n",
        "  # Train classifier\n",
        "  clf = SVC(kernel='linear')\n",
        "  clf.fit(X_tr_hist, y_tr)\n",
        "\n",
        "  # Validate\n",
        "  acc = clf.score(X_val_hist, y_val)\n",
        "  accuracies.append(acc)\n",
        "\n",
        "print(\"Mean CV accuracy:\", np.mean(accuracies))\n",
        "print(\"Std CV accuracy:\", np.std(accuracies))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8KwYKx8TyPq"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "k = 256              # number of clusters\n",
        "# k=3 around 12% (same as normal kmeans)\n",
        "# k=30 around 43.5% with is actually higher than reduced sift features\n",
        "# k = 200 around 55%\n",
        "# k = 2000 around 66% but 22 min training while 200 took 5 min per cv\n",
        "# k=2000 with batch size 10k, 66% with 23 min traing\n",
        "# k=5000 10k size, 69%\n",
        "# Note that with minibatchKmeans you can increase the number of clusters without getting to penalized (in time) by it\n",
        "# They have better scaling for number of clusters than KMeans\n",
        "batch_size = 8000        # minibatch size\n",
        "desc_folder = \"descriptors\"\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "accuracies = []\n",
        "\n",
        "for train_idx, val_idx in skf.split(X_train, y_train):\n",
        "\n",
        "    # --- 1) Build list of descriptor filenames for this fold ---\n",
        "    train_files = [f\"desc_{i}.npy\" for i in train_idx]\n",
        "    val_files   = [f\"desc_{i}.npy\" for i in val_idx]\n",
        "\n",
        "    # --- 2) Train MiniBatchKMeans only on training descriptors ---\n",
        "    kmeans = MiniBatchKMeans(\n",
        "        n_clusters=k,\n",
        "        batch_size=batch_size,\n",
        "        random_state=0\n",
        "    )\n",
        "\n",
        "    batch = []\n",
        "    current_size = 0\n",
        "\n",
        "    for file in train_files:\n",
        "        desc = load_desc_safe(file)\n",
        "\n",
        "        # Ignore empty descriptors\n",
        "        if desc is None or desc.size == 0:\n",
        "          continue\n",
        "\n",
        "        # Append descriptors for streaming\n",
        "        batch.append(desc)\n",
        "        current_size += desc.shape[0]\n",
        "\n",
        "        # If enough descriptors → partial_fit\n",
        "        if current_size >= batch_size:\n",
        "            kmeans.partial_fit(np.vstack(batch))\n",
        "            batch = []\n",
        "            current_size = 0\n",
        "\n",
        "    # Fit final batch if any\n",
        "    if len(batch) > 0:\n",
        "        kmeans.partial_fit(np.vstack(batch))\n",
        "\n",
        "    # --- 3) Function to compute histograms safely ---\n",
        "    def compute_histograms(file_list):\n",
        "        hists = []\n",
        "        for file in file_list:\n",
        "            desc = load_desc_safe(file)\n",
        "            if desc is None:\n",
        "                hist = np.zeros(k, dtype=int)\n",
        "            else:\n",
        "                labels = kmeans.predict(desc)\n",
        "                hist, _ = np.histogram(labels, bins=np.arange(k+1))\n",
        "            hists.append(hist)\n",
        "        return normalize(np.array(hists), norm='l2')\n",
        "\n",
        "    # --- 4) Build BoVW histograms ---\n",
        "    X_tr_hist = compute_histograms(train_files)\n",
        "    X_val_hist = compute_histograms(val_files)\n",
        "\n",
        "    # Label\n",
        "    y_tr = [y_train[i] for i in train_idx]\n",
        "    y_val = [y_train[i] for i in val_idx]\n",
        "\n",
        "    # --- 5) Train your classifier ---\n",
        "    clf = SVC(kernel='linear')\n",
        "    clf.fit(X_tr_hist, y_tr)\n",
        "\n",
        "    acc = clf.score(X_val_hist, y_val)\n",
        "    accuracies.append(acc)\n",
        "    print(\"Fold accuracy:\", acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLNXOgNbvuwB"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "clf = GradientBoostingClassifier(n_estimators=100,random_state=0)\n",
        "clf.fit(X_tr_hist, y_tr)\n",
        "\n",
        "acc = clf.score(X_val_hist, y_val)\n",
        "print(\"Fold accuracy:\", acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9liWx0nT0jg"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier(n_estimators=350, random_state=42)\n",
        "classifier.fit(X_tr_hist, y_tr)\n",
        "acc = classifier.score(X_val_hist, y_val)\n",
        "print(\"Fold accuracy:\", acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LruT3YaH627"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "nb_clf = GaussianNB()\n",
        "nb_clf.fit(X_tr_hist, y_tr)\n",
        "y_pred_nb = nb_clf.predict(X_val_hist)\n",
        "acc_nb = nb_clf.score(X_val_hist, y_val)\n",
        "print(f\"Accuracy Naïve Bayes: {acc_nb}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpjWPepqIEjr"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "softmax_clf = LogisticRegression(\n",
        "    multi_class='multinomial',  # for multiclass classification (Softmax)\n",
        "    solver='lbfgs',\n",
        "    max_iter=500\n",
        ")\n",
        "softmax_clf.fit(X_tr_hist, y_tr)\n",
        "y_pred_softmax = softmax_clf.predict(X_val_hist)\n",
        "acc_softmax = softmax_clf.score(X_val_hist, y_val)\n",
        "print(f\"Accuracy Softmax Regression: {acc_softmax}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3l9VLeggIXIY"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dt_clf = DecisionTreeClassifier(\n",
        "    max_depth=15,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "dt_clf.fit(X_tr_hist, y_tr)\n",
        "y_pred_dt = dt_clf.predict(X_val_hist)\n",
        "acc_dt = dt_clf.score(X_val_hist, y_val)\n",
        "print(f\"Accuracy Decision Tree: {acc_dt}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
