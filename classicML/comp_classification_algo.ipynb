{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGC0I3Bi_yLz"
      },
      "source": [
        "### Data Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDE8NrPXKdZ2",
        "outputId": "26d3c8ce-4446-443b-f90b-6414d6c2f02e"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Dataset directory\n",
        "dataset_dir = \"data/raw\"\n",
        "\n",
        "# Download dataset\n",
        "path = kagglehub.dataset_download(\"jiayuanchengala/aid-scene-classification-datasets\")\n",
        "src_dir = os.path.join(path, \"AID\")\n",
        "\n",
        "# Move only if not already moved\n",
        "if not os.path.exists(dataset_dir):\n",
        "    print(\"Copying dataset to data/raw...\")\n",
        "    # Note that if I move it then kagglehub wont re-download so we can=t keep consistency\n",
        "    shutil.copytree(src_dir, dataset_dir)\n",
        "else:\n",
        "    print(\"Dataset already exists at data/raw\")\n",
        "\n",
        "print(\"✅ Dataset ready at:\", dataset_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOrWQiOhTJMZ"
      },
      "source": [
        "### Manual Data Separation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that this isn't necessary for sklearn since it already includes splits and cross validation.\n",
        "\n",
        "However, for deep learning (Pytorch) you do need to the separation yourself"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hueuKpDe6uw9",
        "outputId": "9c8606c2-04d9-48fe-ce1a-a7acc3c13d4d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "image_paths =  []\n",
        "labels = []\n",
        "source_dir = 'data/raw'\n",
        "categories = [d for d in os.listdir(source_dir)\n",
        "              if os.path.isdir(os.path.join(source_dir, d))]\n",
        "for category in categories:\n",
        "    category_path = os.path.join(source_dir, category)\n",
        "    files = [os.path.join(category_path, f) for f in os.listdir(category_path)\n",
        "             if os.path.isfile(os.path.join(category_path, f))]\n",
        "    image_paths.extend(files)\n",
        "    labels.extend([category] * len(files))\n",
        "\n",
        "print(f\"Found {len(image_paths)} images in {len(categories)} categories\")\n",
        "os.makedirs(\"metrics\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCZYPpJOU1kV",
        "outputId": "a90bb2ff-2ebf-42af-b088-8bf2278beb13"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Source directory containing all the category folders\n",
        "source_dir = 'data/raw'\n",
        "\n",
        "# Create train and test directories\n",
        "train_dir = 'data/train'\n",
        "test_dir = 'data/test'\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "for category in categories:\n",
        "    category_path = os.path.join(source_dir, category)\n",
        "\n",
        "    if not os.path.exists(category_path):\n",
        "        print(f\"Warning: {category} folder not found, skipping...\")\n",
        "        continue\n",
        "\n",
        "    # Get all files in the category folder\n",
        "    files = [f for f in os.listdir(category_path)\n",
        "             if os.path.isfile(os.path.join(category_path, f))]\n",
        "\n",
        "    if len(files) == 0:\n",
        "        print(f\"Warning: {category} folder is empty, skipping...\")\n",
        "        continue\n",
        "\n",
        "    # Shuffle files randomly\n",
        "    random.shuffle(files)\n",
        "\n",
        "    # Calculate split point (90% for train)\n",
        "    split_idx = int(len(files) * 0.9)\n",
        "    train_files = files[:split_idx]\n",
        "    test_files = files[split_idx:]\n",
        "\n",
        "    # Create category subfolders in train and test\n",
        "    train_category_dir = os.path.join(train_dir, category)\n",
        "    test_category_dir = os.path.join(test_dir, category)\n",
        "    os.makedirs(train_category_dir, exist_ok=True)\n",
        "    os.makedirs(test_category_dir, exist_ok=True)\n",
        "\n",
        "    # Move files to train\n",
        "    for file in train_files:\n",
        "        src = os.path.join(category_path, file)\n",
        "        dst = os.path.join(train_category_dir, file)\n",
        "        shutil.copy2(src, dst)\n",
        "\n",
        "    # Move files to test\n",
        "    for file in test_files:\n",
        "        src = os.path.join(category_path, file)\n",
        "        dst = os.path.join(test_category_dir, file)\n",
        "        shutil.copy2(src, dst)\n",
        "\n",
        "    print(f\"{category}: {len(train_files)} files to train, {len(test_files)} files to test\")\n",
        "\n",
        "print(\"\\nSplit complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GCdL4xacn_y",
        "outputId": "00a41d74-9c4b-4cd2-a589-45b40cf0006e"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# This is for cross-validation (Pytorch and Tensorflow need manual separation)\n",
        "# To scikit-learning you onl need to pass the training data and it does the division\n",
        "\n",
        "# Configuration\n",
        "k_folds = 5\n",
        "output_base = 'kfolds'\n",
        "os.makedirs(output_base, exist_ok=True)\n",
        "\n",
        "print(f\"Creating {k_folds}-fold cross-validation split...\")\n",
        "\n",
        "# Collect all files by category\n",
        "category_files = {}\n",
        "for category in categories:\n",
        "    category_path = os.path.join(source_dir, category)\n",
        "    files = [f for f in os.listdir(category_path)\n",
        "             if os.path.isfile(os.path.join(category_path, f))]\n",
        "    random.shuffle(files)  # Shuffle within each category\n",
        "    category_files[category] = files\n",
        "\n",
        "# Create fold assignments for each category\n",
        "fold_assignments = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "for category, files in category_files.items():\n",
        "    n_files = len(files)\n",
        "    fold_size = n_files // k_folds\n",
        "\n",
        "    # Assign files to folds ensuring balanced distribution\n",
        "    for fold_idx in range(k_folds):\n",
        "        start_idx = fold_idx * fold_size\n",
        "        # Last fold gets any remaining files\n",
        "        end_idx = start_idx + fold_size if fold_idx < k_folds - 1 else n_files\n",
        "        fold_assignments[fold_idx][category] = files[start_idx:end_idx]\n",
        "\n",
        "# Create fold directories with class subdirectories only\n",
        "for fold_idx in range(k_folds):\n",
        "    fold_name = f'fold_{fold_idx + 1}'\n",
        "    fold_dir = os.path.join(output_base, fold_name)\n",
        "\n",
        "    # Create class subdirectories in each fold\n",
        "    for category in categories:\n",
        "        category_fold_dir = os.path.join(fold_dir, category)\n",
        "        os.makedirs(category_fold_dir, exist_ok=True)\n",
        "\n",
        "        # Copy the files assigned to this fold for this category\n",
        "        files_for_this_fold = fold_assignments[fold_idx][category]\n",
        "\n",
        "        for file in files_for_this_fold:\n",
        "            src = os.path.join(source_dir, category, file)\n",
        "            dst = os.path.join(category_fold_dir, file)\n",
        "            shutil.copy2(src, dst)\n",
        "\n",
        "    print(f\"\\n{fold_name}:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    total_files = 0\n",
        "    for category in categories:\n",
        "        files_in_fold = fold_assignments[fold_idx][category]\n",
        "        total_files += len(files_in_fold)\n",
        "        print(f\"  {category}: {len(files_in_fold)} files\")\n",
        "\n",
        "    print(f\"  Total in fold: {total_files} files\")\n",
        "\n",
        "print(f\"\\n\\nK-fold split complete!\")\n",
        "print(f\"Output directory: {output_base}/\")\n",
        "print(f\"Structure: fold_1/, fold_2/, ..., fold_{k_folds}/\")\n",
        "print(f\"Each fold contains: {', '.join(categories)} subdirectories with their respective files\")\n",
        "\n",
        "print(f\"\\n\\nK-fold split complete!\")\n",
        "print(f\"Output directory: {output_base}/\")\n",
        "print(f\"Structure: fold_1/, fold_2/, ..., fold_{k_folds}/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGMbkfnZAyFL"
      },
      "source": [
        "### Bag of Visual Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since Bag of Features consist on 4 stages we decide to use classes to not hardcode each possible combination.\n",
        "\n",
        "We separate them into descriptorExtractor class which is meant to be used in the first step of the Bag of Visual Word which is extracting the features.\n",
        "\n",
        "For the clustering step we would usually simply keep always MiniBatchKMeans since it is the most scalable and faster than KMeans with similar accuracy; however, because we are keeping both in a class for a more theoretical approach where we can compare both \n",
        "\n",
        "Then the encoding part consist on how you analyze the clusters given an image. The original implementation is only with histograms, however, there are some alternatives like VLAD or Fisher vector that we also considered to take into account (to know difference between them in both accuracy and memory). Note that they are considered different algorithms and usually not considered part of BoVW but we decided to classify as encoding class because it doesn't break Bag of Features structure\n",
        "\n",
        "Lastly is the pipeline which could be removed and do training and classifier with the encoders and independent classifiers. However, you can see that the best scalable approach in sklearn are pipelines; however, they are not meant to be used for image classification by default, so we adapt them to be able to follow Bag of Features algorithm  while keeping sklearning advantages (which are fitting, predicting and saving/loading models with joblib)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZjoHsdyA04P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
        "from sklearn.preprocessing import normalize\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Optional\n",
        "from enum import Enum\n",
        "import joblib\n",
        "import cv2 as cv\n",
        "\n",
        "\n",
        "class DescriptorType(Enum):\n",
        "    \"\"\"Enum for available descriptor types.\"\"\"\n",
        "    SIFT = \"SIFT\"\n",
        "    ORB = \"ORB\"\n",
        "    AKAZE = \"AKAZE\"\n",
        "\n",
        "\n",
        "class DescriptorExtractor:\n",
        "    \"\"\"\n",
        "    Unified interface for different feature descriptors.\n",
        "\n",
        "    This class handles the quirks of different OpenCV detectors\n",
        "    and provides a consistent interface.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    descriptor_type : str or DescriptorType\n",
        "        Type of descriptor to use ('SIFT', 'ORB', 'AKAZE', etc.)\n",
        "    n_features : int, optional\n",
        "        Maximum number of features to detect (for SIFT, ORB)\n",
        "    **kwargs : dict\n",
        "        Additional parameters for the specific detector\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        descriptor_type: str = 'SIFT',\n",
        "        n_features: Optional[int] = None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        self.descriptor_type = descriptor_type.upper() # Case sensitive\n",
        "        self.n_features = n_features\n",
        "        self.kwargs = kwargs\n",
        "        self.detector = self._create_detector()\n",
        "\n",
        "        # Store descriptor dimensionality\n",
        "        self.descriptor_size = self._get_descriptor_size()\n",
        "\n",
        "    def _create_detector(self):\n",
        "        \"\"\"Create the appropriate OpenCV detector.\"\"\"\n",
        "        dt = self.descriptor_type\n",
        "\n",
        "        if dt == 'SIFT':\n",
        "            if self.n_features is not None:\n",
        "                return cv.SIFT_create(nfeatures=self.n_features, **self.kwargs)\n",
        "            return cv.SIFT_create(**self.kwargs)\n",
        "\n",
        "        elif dt == 'ORB':\n",
        "            if self.n_features is not None:\n",
        "                return cv.ORB_create(nfeatures=self.n_features, **self.kwargs)\n",
        "            return cv.ORB_create(**self.kwargs)\n",
        "\n",
        "        elif dt == 'AKAZE':\n",
        "            # AKAZE doesn't have nfeatures parameter\n",
        "            return cv.AKAZE_create(**self.kwargs)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Unknown descriptor type: {dt}. \"\n",
        "                f\"Available: {[e.value for e in DescriptorType]}\"\n",
        "            )\n",
        "\n",
        "    def _get_descriptor_size(self) -> int:\n",
        "        \"\"\"Get the descriptor dimensionality for each type.\"\"\"\n",
        "        size_map = {\n",
        "            'SIFT': 128,\n",
        "            'ORB': 32,\n",
        "            'AKAZE': 61\n",
        "        }\n",
        "        return size_map.get(self.descriptor_type, 128)\n",
        "\n",
        "    def extract(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        return_keypoints: bool = False\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Extract descriptors from an image array.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        image : np.ndarray\n",
        "            Input image (grayscale or color)\n",
        "        return_keypoints : bool, default=False\n",
        "            If True, return (keypoints, descriptors) tuple\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        descriptors : np.ndarray\n",
        "            Descriptor array of shape (n_keypoints, descriptor_size)\n",
        "            Returns empty array if no keypoints detected\n",
        "        \"\"\"\n",
        "        # Convert to grayscale if needed\n",
        "        if len(image.shape) == 3:\n",
        "            image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Detect and compute\n",
        "        keypoints, descriptors = self.detector.detectAndCompute(image, None)\n",
        "\n",
        "        # Handle no detections\n",
        "        if descriptors is None or len(keypoints) == 0:\n",
        "            descriptors = np.zeros((0, self.descriptor_size), dtype=np.float32)\n",
        "            keypoints = []\n",
        "        else:\n",
        "            # Ensure float32 for consistency\n",
        "            descriptors = descriptors.astype(np.float32)\n",
        "\n",
        "        if return_keypoints:\n",
        "            return keypoints, descriptors\n",
        "        return descriptors\n",
        "\n",
        "    def extract_from_file(\n",
        "        self,\n",
        "        filepath: str,\n",
        "        return_keypoints: bool = False\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Extract descriptors from an image file.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        filepath : str\n",
        "            Path to image file\n",
        "        return_keypoints : bool, default=False\n",
        "            If True, return (keypoints, descriptors) tuple\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        descriptors : np.ndarray\n",
        "            Descriptor array or empty array if file can't be read\n",
        "        \"\"\"\n",
        "        image = cv.imread(str(filepath), cv.IMREAD_GRAYSCALE)\n",
        "\n",
        "        if image is None:\n",
        "            empty_desc = np.zeros((0, self.descriptor_size), dtype=np.float32)\n",
        "            if return_keypoints:\n",
        "                return [], empty_desc\n",
        "            return empty_desc\n",
        "\n",
        "        return self.extract(image, return_keypoints=return_keypoints)\n",
        "\n",
        "    def extract_batch(\n",
        "        self,\n",
        "        images: list,\n",
        "        from_files: bool = False\n",
        "    ) -> list:\n",
        "        \"\"\"\n",
        "        Extract descriptors from multiple images.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        images : list\n",
        "            List of image arrays or file paths\n",
        "        from_files : bool, default=False\n",
        "            If True, treat images as file paths\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        list of np.ndarray\n",
        "            List of descriptor arrays\n",
        "        \"\"\"\n",
        "        if from_files:\n",
        "            return [self.extract_from_file(img) for img in images]\n",
        "        else:\n",
        "            return [self.extract(img) for img in images]\n",
        "\n",
        "    def load_descriptors(self, filepath: str):\n",
        "        \"\"\"Load descriptors from a .npy file.\"\"\"\n",
        "        descriptors = np.load(filepath, allow_pickle=True)\n",
        "        if descriptors is None or descriptors.size == 0:\n",
        "            return None\n",
        "\n",
        "        # guarantee correct shape\n",
        "        descriptors = np.asarray(descriptors, dtype=np.float32)\n",
        "\n",
        "        if descriptors.ndim != 2 or descriptors.shape[1] != 128:\n",
        "            return None\n",
        "\n",
        "        return descriptors\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (\n",
        "            f\"DescriptorExtractor(type={self.descriptor_type}, \"\n",
        "            f\"n_features={self.n_features}, \"\n",
        "            f\"descriptor_size={self.descriptor_size})\"\n",
        "        )\n",
        "\n",
        "\n",
        "class ClusteringAlgorithm(ABC):\n",
        "    \"\"\"Base class for clustering algorithms.\"\"\"\n",
        "\n",
        "    def __init__(self, n_clusters: int, random_state: int = 42):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.random_state = random_state\n",
        "        self.model = None\n",
        "        self._is_fitted = False\n",
        "\n",
        "    @abstractmethod\n",
        "    def fit(self, X: np.ndarray):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def fit_iterative(self, data_loader, load_func: Optional[callable] = None):\n",
        "        pass\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        if not self._is_fitted:\n",
        "            raise ValueError(\"Model must be fitted before predicting.\")\n",
        "        return self.model.predict(X)\n",
        "\n",
        "    @property\n",
        "    def cluster_centers_(self) -> np.ndarray:\n",
        "        if not self._is_fitted:\n",
        "            raise ValueError(\"Model must be fitted first.\")\n",
        "        return self.model.cluster_centers_\n",
        "\n",
        "    def descriptor_batch_generator(self,\n",
        "    data_loader, load_func=None, max_descriptors=10000):\n",
        "        \"\"\"\n",
        "        Yields batches of descriptors with a total number of rows up to max_descriptors.\n",
        "\n",
        "        Args:\n",
        "            data_loader: iterable of items to load descriptors from.\n",
        "            load_func: optional function(item) -> descriptor array\n",
        "            max_descriptors: max total rows per batch\n",
        "\n",
        "        Yields:\n",
        "            np.ndarray of shape (~max_descriptors, n_features)\n",
        "        \"\"\"\n",
        "        batch = []\n",
        "        current_size = 0   # number of descriptors accumulated\n",
        "\n",
        "        for item in data_loader:\n",
        "            data = load_func(item) if load_func else item\n",
        "\n",
        "            # Skip empty or None\n",
        "            if data is None or data.size == 0:\n",
        "                continue\n",
        "\n",
        "            # Ensure 2D\n",
        "            if data.ndim == 1:\n",
        "                data = data.reshape(1, -1)\n",
        "\n",
        "            batch.append(data)\n",
        "            current_size += data.shape[0]\n",
        "\n",
        "            # If batch is full, yield it\n",
        "            if current_size >= max_descriptors:\n",
        "                yield np.vstack(batch)\n",
        "                batch = []\n",
        "                current_size = 0\n",
        "\n",
        "        # Yield remainder\n",
        "        if batch:\n",
        "            yield np.vstack(batch)\n",
        "\n",
        "\n",
        "class MiniBatchKMeansClustering(ClusteringAlgorithm):\n",
        "    \"\"\"MiniBatch KMeans with support for iterative fitting.\"\"\"\n",
        "\n",
        "    def __init__(self, n_clusters: int, batch_size: int = 1024, random_state: int = 42, **kwargs):\n",
        "        super().__init__(n_clusters, random_state)\n",
        "        self.batch_size = batch_size\n",
        "        self.kwargs = kwargs\n",
        "        self.model = MiniBatchKMeans(\n",
        "            n_clusters=n_clusters,\n",
        "            batch_size=batch_size,\n",
        "            random_state=random_state,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    def fit(self, X: np.ndarray):\n",
        "        self.model.fit(X)\n",
        "        self._is_fitted = True\n",
        "        return self\n",
        "\n",
        "    def fit_iterative(\n",
        "        self,\n",
        "        data_loader,\n",
        "        load_func: Optional[callable] = None,\n",
        "        accumulate_batch_size: Optional[int] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Fit iteratively using partial_fit.\n",
        "\n",
        "        Args:\n",
        "            data_loader: Iterable of file paths or data chunks\n",
        "            load_func: Function to load data from paths\n",
        "            accumulate_batch_size: Size to accumulate before partial_fit\n",
        "                                   (defaults to self.batch_size)\n",
        "        \"\"\"\n",
        "        if accumulate_batch_size is None:\n",
        "            accumulate_batch_size = self.batch_size\n",
        "\n",
        "        for batch in self.descriptor_batch_generator(data_loader, load_func, accumulate_batch_size):\n",
        "            self.model.partial_fit(batch)\n",
        "\n",
        "        self._is_fitted = True\n",
        "        return self\n",
        "\n",
        "class KMeansClustering(ClusteringAlgorithm):\n",
        "    \"\"\"Standard KMeans clustering.\"\"\"\n",
        "\n",
        "    def __init__(self, n_clusters: int, random_state: int = 0, **kwargs):\n",
        "        super().__init__(n_clusters, random_state)\n",
        "        self.kwargs = kwargs\n",
        "        self.model = KMeans(\n",
        "            n_clusters=n_clusters,\n",
        "            random_state=random_state,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    def fit(self, X: np.ndarray):\n",
        "        \"\"\"Fit KMeans on all data at once.\"\"\"\n",
        "        self.model.fit(X)\n",
        "        self._is_fitted = True\n",
        "        return self\n",
        "\n",
        "    def fit_iterative(self, data_loader, load_func: Optional[callable] = None):\n",
        "        \"\"\"KMeans doesn't support iterative fitting - loads all data first.\"\"\"\n",
        "        print(\"Warning: KMeans doesn't support true iterative fitting. Loading all data...\")\n",
        "\n",
        "        all_data = []\n",
        "        for item in data_loader:\n",
        "            data = load_func(item) if load_func else item\n",
        "            if data is not None and data.size > 0:\n",
        "                all_data.append(data)\n",
        "\n",
        "        if all_data:\n",
        "            X = np.vstack(all_data)\n",
        "            self.fit(X)\n",
        "        return self\n",
        "\n",
        "\n",
        "class EncodingAlgorithm(ABC):\n",
        "    \"\"\"Base class for encoding algorithms.\"\"\"\n",
        "\n",
        "    def __init__(self, clustering: ClusteringAlgorithm):\n",
        "        self.clustering = clustering\n",
        "\n",
        "    @abstractmethod\n",
        "    def encode(self, descriptors: np.ndarray) -> np.ndarray:\n",
        "        pass\n",
        "\n",
        "\n",
        "class BagOfVisualWords(EncodingAlgorithm):\n",
        "    \"\"\"Bag of Visual Words encoding.\"\"\"\n",
        "\n",
        "    def __init__(self, clustering: ClusteringAlgorithm,\n",
        "                 normalize_hist: bool = True,\n",
        "                 norm_type: str = 'l2'):  # Add this: 'l1' or 'l2'\n",
        "        super().__init__(clustering)\n",
        "        self.normalize_hist = normalize_hist\n",
        "        self.norm_type = norm_type\n",
        "\n",
        "    def encode(self, descriptors: np.ndarray) -> np.ndarray:\n",
        "        if descriptors is None or descriptors.size == 0:\n",
        "            return np.zeros(self.clustering.n_clusters)\n",
        "\n",
        "        labels = self.clustering.predict(descriptors)\n",
        "        histogram = np.bincount(labels, minlength=self.clustering.n_clusters).astype(np.float32)\n",
        "\n",
        "        if self.normalize_hist:\n",
        "            if self.norm_type == 'l2':\n",
        "                norm = np.linalg.norm(histogram)\n",
        "                if norm > 0:\n",
        "                    histogram = histogram / norm\n",
        "            elif self.norm_type == 'l1':\n",
        "                if histogram.sum() > 0:\n",
        "                    histogram = histogram / histogram.sum()\n",
        "\n",
        "        return histogram\n",
        "\n",
        "\n",
        "class VLAD(EncodingAlgorithm):\n",
        "    \"\"\"Vector of Locally Aggregated Descriptors.\"\"\"\n",
        "\n",
        "    def __init__(self, clustering: ClusteringAlgorithm, normalize_vlad: bool = True):\n",
        "        super().__init__(clustering)\n",
        "        self.normalize_vlad = normalize_vlad\n",
        "\n",
        "    def encode(self, descriptors: np.ndarray) -> np.ndarray:\n",
        "        if descriptors is None or descriptors.size == 0:\n",
        "            n_features = self.clustering.cluster_centers_.shape[1]\n",
        "            return np.zeros(self.clustering.n_clusters * n_features)\n",
        "\n",
        "        labels = self.clustering.predict(descriptors)\n",
        "        centers = self.clustering.cluster_centers_\n",
        "        n_clusters, n_features = centers.shape\n",
        "        vlad = np.zeros((n_clusters, n_features))\n",
        "\n",
        "        for idx in range(n_clusters):\n",
        "            mask = (labels == idx)\n",
        "            if np.any(mask):\n",
        "                residuals = descriptors[mask] - centers[idx]\n",
        "                vlad[idx] = residuals.sum(axis=0)\n",
        "\n",
        "        vlad = vlad.flatten()\n",
        "        vlad = vlad.reshape(n_clusters, n_features)\n",
        "        vlad = normalize(vlad, norm='l2', axis=1)\n",
        "        vlad = vlad.flatten()\n",
        "        vlad = np.sign(vlad) * np.sqrt(np.abs(vlad))\n",
        "\n",
        "        if self.normalize_vlad:\n",
        "            vlad = normalize(vlad.reshape(1, -1), norm='l2').flatten()\n",
        "\n",
        "        return vlad\n",
        "\n",
        "# This a wrapper meant it to be compatible with sklearn pre-processing\n",
        "# Since normal sklearn doesn't support complex pipelines with custom fitting/loading and image paths\n",
        "class VisualEncodingTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    sklearn-compatible wrapper for visual encoding pipelines.\n",
        "\n",
        "    This allows you to:\n",
        "    - Save/load with joblib\n",
        "    - Use in sklearn Pipelines\n",
        "    - Use with GridSearchCV\n",
        "    - Keep your flexible architecture\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    clustering : ClusteringAlgorithm\n",
        "        Custom clustering algorithm instance\n",
        "    encoding : EncodingAlgorithm\n",
        "        Custom encoding algorithm instance\n",
        "    descriptor_extractor : callable, optional\n",
        "        Function to extract descriptors from images\n",
        "        Signature: descriptor_extractor(image_path) -> np.ndarray\n",
        "    iterative_fit : bool, default=False\n",
        "        If True, use iterative fitting (for large datasets)\n",
        "    skip_clustering_fit: bool, default=False\n",
        "        If True, skip fitting the clustering model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        clustering: ClusteringAlgorithm,\n",
        "        encoding: EncodingAlgorithm,\n",
        "        descriptor_extractor: Optional[callable] = None,\n",
        "        iterative_fit: bool = False,\n",
        "        skip_clustering_fit: bool = False\n",
        "    ):\n",
        "        self.clustering = clustering\n",
        "        self.encoding = encoding\n",
        "        self.descriptor_extractor = descriptor_extractor\n",
        "        self.iterative_fit = iterative_fit\n",
        "        self.skip_clustering_fit = skip_clustering_fit\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fit the clustering model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like\n",
        "            Either:\n",
        "            - List of image paths (if descriptor_extractor is provided)\n",
        "            - List of descriptor arrays\n",
        "            - Single stacked array of all descriptors\n",
        "        y : array-like, optional\n",
        "            Target labels (unused, for sklearn compatibility)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self\n",
        "        \"\"\"\n",
        "        # Check if clustering is already fitted and we should skip\n",
        "        if self.skip_clustering_fit:\n",
        "            if not self.clustering._is_fitted:\n",
        "                raise ValueError(\n",
        "                    \"skip_clustering_fit=True but clustering is not fitted! \"\n",
        "                    \"Either fit the clustering first or set skip_clustering_fit=False\"\n",
        "                )\n",
        "            print(\"Skipping clustering fit (using pre-fitted clusters)\")\n",
        "            return self\n",
        "\n",
        "        if self.descriptor_extractor is not None:\n",
        "            # X is list of image paths\n",
        "            if self.iterative_fit:\n",
        "                self.clustering.fit_iterative(X, load_func=self.descriptor_extractor)\n",
        "            else:\n",
        "                # Extract all descriptors first\n",
        "                all_descriptors = []\n",
        "                for path in X:\n",
        "                    desc = self.descriptor_extractor(path)\n",
        "                    if desc is not None and desc.size > 0:\n",
        "                        all_descriptors.append(desc)\n",
        "                if all_descriptors:\n",
        "                    all_descriptors = np.vstack(all_descriptors)\n",
        "                    self.clustering.fit(all_descriptors)\n",
        "        else:\n",
        "            # X is already descriptors\n",
        "            if isinstance(X, list):\n",
        "                # List of descriptor arrays - stack them\n",
        "                X = np.vstack([d for d in X if d is not None and d.size > 0])\n",
        "            self.clustering.fit(X)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Encode images/descriptors into fixed-length vectors.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like\n",
        "            Either:\n",
        "            - List of image paths (if descriptor_extractor is provided)\n",
        "            - List of descriptor arrays (each image's descriptors)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray\n",
        "            Encoded vectors, shape (n_samples, encoding_dim)\n",
        "        \"\"\"\n",
        "        if self.descriptor_extractor is not None:\n",
        "            # X is list of image paths\n",
        "            descriptors_list = [self.descriptor_extractor(path) for path in X]\n",
        "        else:\n",
        "            # X is already list of descriptor arrays\n",
        "            descriptors_list = X\n",
        "\n",
        "        # Encode each image's descriptors\n",
        "        encoded = []\n",
        "        for descriptors in descriptors_list:\n",
        "            vector = self.encoding.encode(descriptors)\n",
        "            encoded.append(vector)\n",
        "\n",
        "        return np.array(encoded)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        \"\"\"Get parameters for sklearn compatibility.\"\"\"\n",
        "        params = {\n",
        "            'clustering': self.clustering,\n",
        "            'encoding': self.encoding,\n",
        "            'descriptor_extractor': self.descriptor_extractor,\n",
        "            'iterative_fit': self.iterative_fit\n",
        "        }\n",
        "\n",
        "        if deep and hasattr(self.clustering, 'get_params'):\n",
        "            # Include clustering params with prefix\n",
        "            cluster_params = self.clustering.__dict__.copy()\n",
        "            params.update({f'clustering__{k}': v for k, v in cluster_params.items()})\n",
        "\n",
        "        return params\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        \"\"\"Set parameters for sklearn compatibility.\"\"\"\n",
        "        for key, value in params.items():\n",
        "            if '__' in key:\n",
        "                # Handle nested parameters like 'clustering__n_clusters'\n",
        "                obj_name, param_name = key.split('__', 1)\n",
        "                obj = getattr(self, obj_name)\n",
        "                setattr(obj, param_name, value)\n",
        "            else:\n",
        "                setattr(self, key, value)\n",
        "        return self\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Convenience Factory Functions\n",
        "# ============================================================================\n",
        "\n",
        "def create_bovw_pipeline(\n",
        "    n_clusters: int = 512,\n",
        "    batch_size: int = 1024,\n",
        "    normalize: bool = True,\n",
        "    descriptor_extractor: Optional[DescriptorExtractor] = None,  # Changed type hint\n",
        "    iterative_fit: bool = False,\n",
        "    clustering: Optional[ClusteringAlgorithm] = None  # NEW: optional pre-fitted\n",
        ") -> VisualEncodingTransformer:\n",
        "    \"\"\"\n",
        "    Create a BoVW encoding pipeline.\n",
        "\n",
        "    Example:\n",
        "        >>> extractor = DescriptorExtractor.create_sift(n_features=500)\n",
        "        >>> bovw = create_bovw_pipeline(\n",
        "        ...     n_clusters=1024,\n",
        "        ...     descriptor_extractor=extractor.extract_from_file\n",
        "        ... )\n",
        "        >>> bovw.fit(train_image_paths)\n",
        "    \"\"\"\n",
        "\n",
        "    if clustering is None:\n",
        "        clustering = MiniBatchKMeansClustering(\n",
        "            n_clusters=n_clusters,\n",
        "            batch_size=batch_size,\n",
        "            random_state=42\n",
        "        )\n",
        "        skip_fit = False\n",
        "    else:\n",
        "        # Check if already fitted\n",
        "        skip_fit = clustering._is_fitted\n",
        "        if skip_fit:\n",
        "            print(f\"Using pre-fitted clustering with {clustering.n_clusters} clusters\")\n",
        "\n",
        "    encoding = BagOfVisualWords(clustering, normalize_hist=normalize)\n",
        "\n",
        "    # Convert DescriptorExtractor to callable if provided\n",
        "    extract_func = None\n",
        "    if descriptor_extractor is not None:\n",
        "        if isinstance(descriptor_extractor, DescriptorExtractor):\n",
        "            extract_func = descriptor_extractor.extract_from_file\n",
        "        else:\n",
        "            extract_func = descriptor_extractor\n",
        "\n",
        "    return VisualEncodingTransformer(\n",
        "        clustering=clustering,\n",
        "        encoding=encoding,\n",
        "        descriptor_extractor=extract_func,\n",
        "        iterative_fit=iterative_fit,\n",
        "        skip_clustering_fit=skip_fit\n",
        "    )\n",
        "\n",
        "\n",
        "def create_vlad_pipeline(\n",
        "    n_clusters: int = 256,\n",
        "    batch_size: int = 1024,\n",
        "    normalize: bool = True,\n",
        "    descriptor_extractor: Optional[DescriptorExtractor] = None,  # Changed type hint\n",
        "    iterative_fit: bool = False,\n",
        "    clustering: Optional[ClusteringAlgorithm] = None  # NEW: optional pre-fitted\n",
        ") -> VisualEncodingTransformer:\n",
        "    \"\"\"Create a VLAD encoding pipeline.\"\"\"\n",
        "    if clustering is None:\n",
        "        clustering = MiniBatchKMeansClustering(\n",
        "            n_clusters=n_clusters,\n",
        "            batch_size=batch_size,\n",
        "            random_state=42\n",
        "        )\n",
        "        skip_fit = False\n",
        "    else:\n",
        "        # Check if already fitted\n",
        "        skip_fit = clustering._is_fitted\n",
        "        if skip_fit:\n",
        "            print(f\"Using pre-fitted clustering with {clustering.n_clusters} clusters\")\n",
        "    encoding = VLAD(clustering, normalize_vlad=normalize)\n",
        "\n",
        "    # Convert DescriptorExtractor to callable if provided\n",
        "    extract_func = None\n",
        "    if descriptor_extractor is not None:\n",
        "        if isinstance(descriptor_extractor, DescriptorExtractor):\n",
        "            extract_func = descriptor_extractor.extract_from_file\n",
        "        else:\n",
        "            extract_func = descriptor_extractor\n",
        "            print(\"dx\")\n",
        "\n",
        "    return VisualEncodingTransformer(\n",
        "        clustering=clustering,\n",
        "        encoding=encoding,\n",
        "        descriptor_extractor=extract_func,\n",
        "        iterative_fit=iterative_fit,\n",
        "        skip_clustering_fit=skip_fit\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvDO7tPCTiTM"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic structure/Standard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEk296AuPYxq"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "le = LabelEncoder()\n",
        "# It is easier for metrics and evaluation to have encoded labels (numeric instead of strings)\n",
        "labels_encoded = le.fit_transform(labels)\n",
        "\n",
        "sift = DescriptorExtractor(descriptor_type='SIFT')\n",
        "output_dir = \"descriptors\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "for x in (image_paths):\n",
        "  p = Path(x)\n",
        "  descriptors = sift.extract_from_file(x)\n",
        "\n",
        "  np.save(f\"{output_dir}/{p.stem}.npy\", descriptors)\n",
        "\n",
        "# To test different algorithms we saved the descriptors to only extract them once\n",
        "image_paths = [f\"{output_dir}/{Path(x).stem}.npy\" for x in image_paths]\n",
        "\n",
        "# Note that stratify makes sure that the class distribution is maintained in train and test splits (important in classification tasks)\n",
        "X_train, X_test, y_train, y_test = train_test_split(image_paths, labels_encoded, test_size=0.2, random_state=0, stratify=labels)\n",
        "\n",
        "\n",
        "# Save and load\n",
        "# joblib.dump(full_pipeline, 'image_classifier.pkl')\n",
        "# loaded_pipeline = joblib.load('image_classifier.pkl')\n",
        "# predictions = loaded_pipeline.predict(test_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross-Validation / Tuning Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This part is only meant to be run for calibrating and tuning the hyperparameters of models (like number of clusters, generalization (C), max-depth...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sklearn-compatible transformer\n",
        "\n",
        "# The one for cross-validation we need to fit in every fold (else it would leak data)\n",
        "bovw_cv = create_bovw_pipeline(\n",
        "    n_clusters=2048,\n",
        "    descriptor_extractor=sift.load_descriptors,\n",
        "    iterative_fit=True\n",
        ")\n",
        "\n",
        "# For single validation we could proceed to fit directly and re-separating the training data\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=0, stratify=y_train)\n",
        "\n",
        "# For this case we can simply fit one the whole training data and compare with the single validation fold\n",
        "# clustering = MiniBatchKMeansClustering(\n",
        "#     n_clusters=2048,\n",
        "#     batch_size=10000\n",
        "# )\n",
        "\n",
        "# clustering.fit_iterative(\n",
        "#     data_loader=X_train,\n",
        "#     load_func=sift.load_descriptors\n",
        "# )\n",
        "\n",
        "# With this approach we highly reduce training time since we don't need to constantly fit the clustering in every fold\n",
        "# bovw = create_bovw_pipeline(\n",
        "#     clustering=clustering,\n",
        "#     descriptor_extractor=sift.load_descriptors\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_validation, StratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('encoding', bovw_cv),\n",
        "    ('classifier',  # You can change the classifier here\n",
        "     # For example SVM\n",
        "     SVC(kernel='linear', C=1.0, random_state=42)\n",
        "    )\n",
        "])\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_results = cross_validation(\n",
        "    pipeline, \n",
        "    X_train, \n",
        "    y_train, \n",
        "    cv=cv,\n",
        "    scoring='accuracy',\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "for fold in range(5):\n",
        "    print(f\"Fold {fold+1}: Train={cv_results['train_score'][fold]:.4f}, \"\n",
        "          f\"Val={cv_results['test_score'][fold]:.4f}\")\n",
        "\n",
        "print(f\"\\nMean Train: {cv_results['train_score'].mean():.4f} ± {cv_results['train_score'].std():.4f}\")\n",
        "print(f\"Mean Val:   {cv_results['test_score'].mean():.4f} ± {cv_results['test_score'].std():.4f}\")\n",
        "print(f\"Overfit:    {cv_results['train_score'].mean() - cv_results['test_score'].mean():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross validation can also be done manually where you see each fold explicitly\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "accuracies = []\n",
        "\n",
        "for train_idx, val_idx in skf.split(X_train, y_train):\n",
        "    X_tr, X_val = [X_train[i] for i in train_idx], [X_train[i] for i in val_idx]\n",
        "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "    # Fit the pipeline\n",
        "    pipeline.fit(X_tr, y_tr)\n",
        "\n",
        "    # Evaluate\n",
        "    test_accuracy = pipeline.score(X_val, y_val)\n",
        "    train_accuracy = pipeline.score(X_tr, y_tr)\n",
        "    accuracies.append([train_accuracy, test_accuracy])\n",
        "    print(f\"Fold Train accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"Fold Validation Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nCross-validation results:\")\n",
        "accuracies = np.array(accuracies)\n",
        "print(f\"Mean Train: {accuracies[:,0].mean():.4f} ± {accuracies[:,0].std():.4f}\")\n",
        "print(f\"Mean Val: {accuracies[:,1].mean():.4f} ± {accuracies[:,1].std():.4f}\")\n",
        "print(f\"Overfit: {accuracies[:,0].mean() - accuracies[:,1].mean():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Production Ready"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This part is only meant to be run once after you have already defined the best hyperparameter for the models\n",
        "\n",
        "It generates metrics data and safe models pipeline to be used to predict other dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD5qTazADKJD"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ciby5b4oDSn4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        ")\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.pipeline import Pipeline\n",
        "import time\n",
        "import joblib\n",
        "\n",
        "\n",
        "def evaluate_model(\n",
        "    pipeline, model_name,\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    le,\n",
        "    verbose=True,\n",
        "    save_model_path=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Train and evaluate a single model.\n",
        "    Returns: dict of metrics, predictions, pipeline, and timing info.\n",
        "    \"\"\"\n",
        "\n",
        "    # Train\n",
        "    if verbose:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Training: {model_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    train_time = time.time() - start_time\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"✓ Trained in {train_time:.2f}s\")\n",
        "\n",
        "    # Predict\n",
        "    y_train_pred = pipeline.predict(X_train)\n",
        "    y_test_pred = pipeline.predict(X_test)\n",
        "    inference_time = (time.time() - start_time) / len(X_test)\n",
        "\n",
        "    # Metrics\n",
        "    train_acc = accuracy_score(y_train, y_train_pred)\n",
        "    test_acc  = accuracy_score(y_test, y_test_pred)\n",
        "    precision = precision_score(y_test, y_test_pred, average='macro', zero_division=0)\n",
        "    recall    = recall_score(y_test, y_test_pred, average='macro', zero_division=0)\n",
        "    f1        = f1_score(y_test, y_test_pred, average='macro', zero_division=0)\n",
        "    overfit   = train_acc - test_acc\n",
        "\n",
        "    # Try probabilities & ROC-AUC\n",
        "    try:\n",
        "        y_proba = pipeline.predict_proba(X_test)\n",
        "        roc_auc = roc_auc_score(y_test, y_proba, multi_class='ovr', average='macro')\n",
        "    except Exception:\n",
        "        y_proba = None\n",
        "        roc_auc = np.nan\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "    results = {\n",
        "        'model_name': model_name,\n",
        "        'pipeline': pipeline,\n",
        "        'train_time': train_time,\n",
        "        'inference_time_ms': inference_time * 1000,\n",
        "        'train_accuracy': train_acc,\n",
        "        'test_accuracy': test_acc,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'overfit': overfit,\n",
        "        'y_test': y_test,\n",
        "        'y_pred': y_test_pred,\n",
        "        'y_proba': y_proba,\n",
        "        'confusion_matrix': cm,\n",
        "        'X_test': X_test,\n",
        "        'label_encoder': le\n",
        "    }\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n{'─'*60}\")\n",
        "        print(f\"  {model_name} - Summary\")\n",
        "        print(f\"{'─'*60}\")\n",
        "        print(f\"  Train Acc:   {train_acc:.4f}\")\n",
        "        print(f\"  Test Acc:    {test_acc:.4f}\")\n",
        "        print(f\"  Precision:   {precision:.4f}\")\n",
        "        print(f\"  Recall:      {recall:.4f}\")\n",
        "        print(f\"  F1:          {f1:.4f}\")\n",
        "        if not np.isnan(roc_auc):\n",
        "            print(f\"  ROC-AUC:     {roc_auc:.4f}\")\n",
        "        print(f\"  Overfit:     {overfit:.4f}\")\n",
        "        print(f\"  Train Time:  {train_time:.2f}s\")\n",
        "        print(f\"  Inference:   {inference_time * 1000:.3f}ms/sample\")\n",
        "        print(f\"{'─'*60}\\n\")\n",
        "\n",
        "    # Save model if requested\n",
        "    if save_model_path:\n",
        "        joblib.dump(pipeline, save_model_path)\n",
        "        if verbose:\n",
        "            print(f\"✓ Model saved to {save_model_path}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def plot_confusion_matrix(results, normalize=True, figsize=(10, 8), save_path=None):\n",
        "    cm = results['confusion_matrix']\n",
        "    le = results['label_encoder']\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        fmt = '.2f'\n",
        "        title = f\"{results['model_name']}\\nNormalized Confusion Matrix\"\n",
        "    else:\n",
        "        fmt = 'd'\n",
        "        title = f\"{results['model_name']}\\nConfusion Matrix\"\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.heatmap(cm, annot=True, fmt=fmt, cmap='Blues',\n",
        "                xticklabels=le.classes_,\n",
        "                yticklabels=le.classes_,\n",
        "                cbar_kws={'label': 'Proportion' if normalize else 'Count'})\n",
        "    plt.title(title, fontsize=14, fontweight='bold', pad=20)\n",
        "    plt.xlabel('Predicted Label', fontweight='bold')\n",
        "    plt.ylabel('True Label', fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"✓ Saved to {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_roc_curve(results, figsize=(12, 5), save_path=None):\n",
        "    y_proba = results['y_proba']\n",
        "    if y_proba is None:\n",
        "        print(\"⚠ Skipping ROC: model doesn’t support predict_proba()\")\n",
        "        return\n",
        "\n",
        "    y_test = results['y_test']\n",
        "    le = results['label_encoder']\n",
        "    n_classes = len(le.classes_)\n",
        "    y_test_bin = label_binarize(y_test, classes=range(n_classes))\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "\n",
        "    # Compute per-class ROC\n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_proba[:, i])\n",
        "\n",
        "    # Macro-average ROC\n",
        "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "    mean_tpr = np.zeros_like(all_fpr)\n",
        "    for i in range(n_classes):\n",
        "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
        "    mean_tpr /= n_classes\n",
        "    macro_auc = np.trapz(mean_tpr, all_fpr)\n",
        "\n",
        "    # Plot\n",
        "    ax = axes[0]\n",
        "    ax.plot(all_fpr, mean_tpr, linewidth=3, label=f'Macro-avg (AUC={macro_auc:.3f})')\n",
        "    ax.plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Random')\n",
        "    ax.set_xlabel('FPR', fontweight='bold')\n",
        "    ax.set_ylabel('TPR', fontweight='bold')\n",
        "    ax.set_title(f\"{results['model_name']}\\nMacro-Average ROC\", fontweight='bold')\n",
        "    ax.legend(loc='lower right')\n",
        "    ax.grid(alpha=0.3)\n",
        "\n",
        "    # Per-class (top 10)\n",
        "    ax = axes[1]\n",
        "    for i in range(min(n_classes, 10)):\n",
        "        auc_i = np.trapz(tpr[i], fpr[i])\n",
        "        ax.plot(fpr[i], tpr[i], label=f'{le.classes_[i]} ({auc_i:.2f})', linewidth=1.5)\n",
        "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
        "    ax.set_xlabel('FPR', fontweight='bold')\n",
        "    ax.set_ylabel('TPR', fontweight='bold')\n",
        "    ax.set_title(f\"{results['model_name']}\\nPer-Class ROC\", fontweight='bold')\n",
        "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=8)\n",
        "    ax.grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"✓ Saved to {save_path}\")\n",
        "    plt.show()\n",
        "\n",
        "def print_classification_report(results):\n",
        "    y_test, y_pred = results['y_test'], results['y_pred']\n",
        "    le = results['label_encoder']\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Classification Report: {results['model_name']}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    print(classification_report(y_test, y_pred, target_names=le.classes_, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "models_dir = \"models\"\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "clustering = MiniBatchKMeansClustering(\n",
        "    n_clusters=2048,\n",
        "    batch_size=10000\n",
        ")\n",
        "\n",
        "clustering.fit_iterative(\n",
        "    data_loader=X_train,\n",
        "    load_func=sift.load_descriptors\n",
        ")\n",
        "\n",
        "# With this approach we highly reduce training time since we don't need to constantly fit the clustering in every fold\n",
        "bovw = create_bovw_pipeline(\n",
        "    clustering=clustering,\n",
        "    descriptor_extractor=sift.load_descriptors\n",
        ")\n",
        "\n",
        "vlad = create_vlad_pipeline(\n",
        "    clustering=clustering,\n",
        "    descriptor_extractor=sift.load_descriptors\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q5UIeqwDdgp"
      },
      "source": [
        "### Naive Gaussian Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3FMEhelDcqB"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import Normalizer\n",
        "bovw_gnb_pipeline = Pipeline([\n",
        "        ('encoding', bovw),\n",
        "        ('classifier', GaussianNB())\n",
        "])\n",
        "\n",
        "results_bovw_gnb = evaluate_model(\n",
        "        bovw_gnb_pipeline,\n",
        "        model_name=\"BoVW + GaussianNB\",\n",
        "        X_train=X_train,\n",
        "        y_train=y_train,\n",
        "        X_test=X_test,\n",
        "        y_test=y_test,\n",
        "        le=le,\n",
        "        verbose=True,\n",
        "        save_model_path=f\"{models_dir}/bovw_gnb_model.pkl\"\n",
        "        )\n",
        "plot_confusion_matrix(results_bovw_gnb, normalize=True, save_path=\"metrics/bovw_gnb_confusion_matrix.png\")\n",
        "plot_roc_curve(results_bovw_gnb, save_path=\"metrics/bovw_gnb_roc_curve.png\")\n",
        "print_classification_report(results_bovw_gnb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "bovw_softmax_pipeline = Pipeline([\n",
        "        ('encoding', bovw),\n",
        "        ('classifier', LogisticRegression( multi_class='multinomial',  # for multiclass classification (Softmax)\n",
        "            solver='lbfgs',\n",
        "            max_iter=500\n",
        "))\n",
        "])\n",
        "\n",
        "results_bovw_softmax = evaluate_model(\n",
        "        bovw_softmax_pipeline,\n",
        "        model_name=\"BoVW + Softmax\",\n",
        "        X_train=X_train,\n",
        "        y_train=y_train,\n",
        "        X_test=X_test,\n",
        "        y_test=y_test,\n",
        "        le=le,\n",
        "        verbose=True,\n",
        "        save_model_path=models_dir + \"/bovw_softmax_model.pkl\"\n",
        "        )\n",
        "plot_confusion_matrix(results_bovw_softmax, normalize=True, save_path=\"metrics/bovw_softmax_confusion_matrix.png\")\n",
        "plot_roc_curve(results_bovw_softmax, save_path=\"metrics/bovw_softmax_roc_curve.png\")\n",
        "print_classification_report(results_bovw_softmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Decisition Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bovw_dt_pipeline = Pipeline([\n",
        "        ('encoding', bovw),\n",
        "        ('normalizer', Normalizer(norm='l2'))\n",
        "        ('classifier', DecisionTreeClassifier(\n",
        "            max_depth=15,\n",
        "            random_state=42\n",
        "        ))\n",
        "])\n",
        "\n",
        "results_bovw_dt = evaluate_model(\n",
        "        bovw_dt_pipeline,\n",
        "        model_name=\"BoVW + Decision Tree\",\n",
        "        X_train=X_train,\n",
        "        y_train=y_train,\n",
        "        X_test=X_test,\n",
        "        y_test=y_test,\n",
        "        le=le,\n",
        "        verbose=True,\n",
        "        )\n",
        "plot_confusion_matrix(results_bovw_dt, normalize=True, save_path=\"metrics/bovw_dt_confusion_matrix.png\")\n",
        "plot_roc_curve(results_bovw_dt, save_path=\"metrics/bovw_dt_roc_curve.png\")\n",
        "print_classification_report(results_bovw_dt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "bovw_rf_pipeline = Pipeline([\n",
        "        ('encoding', bovw),\n",
        "        ('classifier', RandomForestClassifier(n_estimators=350, random_state=42))\n",
        "])\n",
        "\n",
        "results_bovw_rf = evaluate_model(\n",
        "        bovw_rf_pipeline,\n",
        "        model_name=\"BoVW + Random Forest\",\n",
        "        X_train=X_train,\n",
        "        y_train=y_train,\n",
        "        X_test=X_test,\n",
        "        y_test=y_test,\n",
        "        le=le,\n",
        "        verbose=True,\n",
        "        save_model_path=f\"{models_dir}/bovw_rf_model.pkl\"\n",
        "        )\n",
        "plot_confusion_matrix(results_bovw_rf, normalize=True, save_path=\"metrics/bovw_rf_confusion_matrix.png\")\n",
        "plot_roc_curve(results_bovw_rf, save_path=\"metrics/bovw_rf_roc_curve.png\")\n",
        "print_classification_report(results_bovw_rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "# Use in sklearn Pipeline\n",
        "bovw_svc_lin_pipeline = Pipeline([\n",
        "        ('encoding', bovw),\n",
        "        ('classifier', SVC(kernel='linear', C=10.0, probability=True))\n",
        "])\n",
        "    \n",
        "\n",
        "results_bovw_svc_lin = evaluate_model(\n",
        "        bovw_svc_lin_pipeline,\n",
        "        model_name=\"BoVW + Support Vector Classifier (Linear)\",\n",
        "        X_train=X_train,\n",
        "        y_train=y_train,\n",
        "        X_test=X_test,\n",
        "        y_test=y_test,\n",
        "        le=le,\n",
        "        verbose=True,\n",
        "        save_model_path=f\"{models_dir}/bovw_svc_lin_model.pkl\"\n",
        "        )\n",
        "plot_confusion_matrix(results_bovw_svc_lin, normalize=True, save_path=\"metrics/bovw_svc_lin_confusion_matrix.png\")\n",
        "plot_roc_curve(results_bovw_svc_lin, save_path=\"metrics/bovw_svc_lin_roc_curve.png\")\n",
        "print_classification_report(results_bovw_svc_lin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "# Use in sklearn Pipeline\n",
        "bovw_svc_rbf_pipeline = Pipeline([\n",
        "        ('encoding', bovw),\n",
        "        ('classifier', SVC(kernel='rbf', C=10.0, gamma='scale', probability=True))\n",
        "])\n",
        "    \n",
        "\n",
        "results_bovw_svc_rbf = evaluate_model(\n",
        "        bovw_svc_rbf_pipeline,\n",
        "        model_name=\"BoVW + Support Vector Classifier (RBF)\",\n",
        "        X_train=X_train,\n",
        "        y_train=y_train,\n",
        "        X_test=X_test,\n",
        "        y_test=y_test,\n",
        "        le=le,\n",
        "        verbose=True,\n",
        "        save_model_path=f\"{models_dir}/bovw_svc_rbf_model.pkl\"\n",
        "        )\n",
        "plot_confusion_matrix(results_bovw_svc_rbf, normalize=True, save_path=\"metrics/bovw_svc_rbf_confusion_matrix.png\")\n",
        "plot_roc_curve(results_bovw_svc_rbf, save_path=\"metrics/bovw_svc_rbf_roc_curve.png\")\n",
        "print_classification_report(results_bovw_svc_rbf)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
